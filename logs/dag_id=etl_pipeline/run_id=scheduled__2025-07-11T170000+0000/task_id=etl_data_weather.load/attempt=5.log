{"timestamp":"2025-07-11T17:37:53.848988","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-11T17:37:53.850123","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:37:54.958008Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:37:57.607630Z","level":"error","event":"25/07/11 17:37:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:37:57.904831Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:37:57.905585Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:37:57.905989Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.134133Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.134597Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.134892Z","level":"info","event":"Current task name:etl_data_weather.load","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.135158Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.135424Z","level":"info","event":"extract data from file","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.167444Z","level":"info","event":"spark.hadoop.fs.s3a.connection.part.upload.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.167982Z","level":"info","event":"spark.hadoop.fs.s3a.socket.recv.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.168315Z","level":"info","event":"spark.hadoop.fs.s3a.impl.disable.cache = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.168604Z","level":"info","event":"spark.hadoop.fs.s3a.connection.request.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.168904Z","level":"info","event":"spark.hadoop.fs.s3a.path.style.access = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.169173Z","level":"info","event":"spark.rdd.compress = True","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.169414Z","level":"info","event":"spark.hadoop.fs.s3a.connection.idle.time = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.169669Z","level":"info","event":"spark.executor.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.169998Z","level":"info","event":"spark.driver.host = 8344c87913bd","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.170300Z","level":"info","event":"spark.hadoop.fs.s3a.endpoint = http://127.0.0.1:9000/","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.170689Z","level":"info","event":"spark.app.submitTime = 1752255477793","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.171073Z","level":"info","event":"spark.hadoop.fs.s3a.connection.establish.timeout = 5000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.171350Z","level":"info","event":"spark.hadoop.fs.s3a.access.key = admin","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.171579Z","level":"info","event":"spark.hadoop.fs.s3a.retry.interval = 500","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.171800Z","level":"info","event":"spark.jars = /opt/spark/jars/hadoop-aws-3.3.6.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,/opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.172049Z","level":"info","event":"spark.hadoop.fs.s3a.connection.maximum = 15","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.172323Z","level":"info","event":"spark.hadoop.fs.s3a.retry.limit = 5","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.172560Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2097152","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.172827Z","level":"info","event":"spark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.173063Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.min.seek.size = 131072","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.173320Z","level":"info","event":"spark.hadoop.fs.s3a.secret.key = admin12345","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.173600Z","level":"info","event":"spark.sql.adaptive.coalescePartitions.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.173859Z","level":"info","event":"spark.hadoop.fs.s3a.socket.send.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.174099Z","level":"info","event":"spark.hadoop.fs.s3a.fast.upload = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.174359Z","level":"info","event":"spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.174616Z","level":"info","event":"spark.sql.artifact.isolation.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.174839Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.size = 104857600","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.175074Z","level":"info","event":"spark.master = local[*]","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.175322Z","level":"info","event":"spark.executor.id = driver","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.175598Z","level":"info","event":"spark.driver.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.175868Z","level":"info","event":"spark.app.name = BAITEST","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.176104Z","level":"info","event":"spark.driver.port = 42887","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.176314Z","level":"info","event":"spark.submit.deployMode = client","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.176544Z","level":"info","event":"spark.hadoop.fs.s3a.connection.ssl.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.176788Z","level":"info","event":"spark.serializer.objectStreamReset = 100","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.177025Z","level":"info","event":"spark.ui.showConsoleProgress = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.177253Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.purge.age = 86400","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.177540Z","level":"info","event":"spark.app.id = local-1752255479356","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.177776Z","level":"info","event":"spark.app.startTime = 1752255478287","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.178007Z","level":"info","event":"spark.app.initial.jar.urls = spark://8344c87913bd:42887/jars/hadoop-aws-3.3.6.jar,spark://8344c87913bd:42887/jars/aws-java-sdk-bundle-1.12.787.jar,spark://8344c87913bd:42887/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.178275Z","level":"info","event":"spark.hadoop.fs.s3a.threads.max = 10","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.178473Z","level":"info","event":"spark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.178674Z","level":"info","event":"spark.submit.pyFiles =","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.178936Z","level":"info","event":"spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.179292Z","level":"info","event":"spark.hadoop.fs.s3a.attempts.maximum = 3","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.179626Z","level":"info","event":"spark.hadoop.fs.s3a.threads.keepalivetime = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.179841Z","level":"info","event":"spark.repl.local.jars = file:///opt/spark/jars/hadoop-aws-3.3.6.jar,file:///opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,file:///opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.180106Z","level":"info","event":"spark.sql.adaptive.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:04.180372Z","level":"info","event":"spark.hadoop.fs.s3a.connection.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.828977Z","level":"error","event":"25/07/11 17:38:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.916080Z","level":"error","event":"25/07/11 17:38:05 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.916668Z","level":"error","event":"You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.917059Z","level":"error","event":"This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.917425Z","level":"error","event":"The AWS SDK for Java 1.x is being used here:","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.917734Z","level":"error","event":"at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.918020Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.918289Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.918546Z","level":"error","event":"at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.918841Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1251)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.919254Z","level":"error","event":"at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.919556Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:982)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.919865Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.920086Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.920299Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.920547Z","level":"error","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.920802Z","level":"error","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.921014Z","level":"error","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.921243Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.921440Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.921629Z","level":"error","event":"at scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.922007Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.922226Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.922419Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.922607Z","level":"error","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.922829Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.923036Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.923230Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.923409Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.923606Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.923826Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.924016Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.924277Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.924488Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.924721Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.924902Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.925087Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.925300Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.925516Z","level":"error","event":"at scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.925717Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.926041Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.926295Z","level":"error","event":"at scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.926556Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.926852Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.927203Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.927429Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.927615Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.927795Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.927977Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.928187Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.928418Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.928666Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.928849Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.929038Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.929274Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.929485Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.929725Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.929931Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.930147Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.930343Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.930533Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.930751Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.930978Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.931171Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.931394Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.931582Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.931764Z","level":"error","event":"at scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.932017Z","level":"error","event":"at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.932238Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.932477Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.932695Z","level":"error","event":"at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.932882Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.933106Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.933293Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.933531Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.933716Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.933897Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.934086Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.934276Z","level":"error","event":"at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.934560Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.934875Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.935207Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.935464Z","level":"error","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.935698Z","level":"error","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.935927Z","level":"error","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.936161Z","level":"error","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.936389Z","level":"error","event":"at py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.936613Z","level":"error","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.936856Z","level":"error","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.937118Z","level":"error","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.937366Z","level":"error","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:05.937640Z","level":"error","event":"at java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.199355Z","level":"error","event":"25/07/11 17:38:42 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://weather-data/opt/airflow/dataset/weather_data.json.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.200002Z","level":"error","event":"org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.200443Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.200811Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.201151Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.201461Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.201743Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$25(S3AFileSystem.java:3629)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.202026Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.202299Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.202675Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.202936Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.203204Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.203473Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3627)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.203708Z","level":"error","event":"\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.203977Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.204263Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.204558Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.204913Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.205228Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.205493Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.205779Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.206035Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.206301Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.206565Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.206811Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.207026Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.207231Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.207454Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.207723Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.207973Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.208192Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.208416Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.208644Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.208881Z","level":"error","event":"\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.209209Z","level":"error","event":"\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.209463Z","level":"error","event":"\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.209862Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.210264Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.210536Z","level":"error","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.210865Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.211203Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.211464Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.211827Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.212415Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.212885Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.213595Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.214004Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.214450Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.214907Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.215505Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.216871Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.218044Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.220612Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.221380Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.224329Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.224959Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.225802Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.226249Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.227530Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.228108Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.228512Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.228877Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.229396Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.229842Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.230296Z","level":"error","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.230758Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.231364Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.231917Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.232471Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.232868Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.233303Z","level":"error","event":"\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.234660Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.235579Z","level":"error","event":"\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.238531Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.239438Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.240501Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.241122Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.241557Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.241971Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.242777Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.243541Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.243903Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.244247Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.244605Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.244961Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.245475Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.246048Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.246392Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.246709Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.247184Z","level":"error","event":"Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.247523Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.247816Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.250400Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.253628Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.255078Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.256704Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.257236Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.257719Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.258653Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.259523Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.260512Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.261125Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.261717Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.262085Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.262467Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.263764Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.264339Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.264735Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.265182Z","level":"error","event":"\t... 84 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.265906Z","level":"error","event":"Caused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.266330Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.267004Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.267479Z","level":"error","event":"\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.268023Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.268496Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.269153Z","level":"error","event":"\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.269491Z","level":"error","event":"\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.269862Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.270298Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.270874Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.271283Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.271971Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.272598Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.273375Z","level":"error","event":"\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.274110Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.274917Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.275292Z","level":"error","event":"\t... 100 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.275613Z","level":"error","event":"Caused by: java.net.ConnectException: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.276396Z","level":"error","event":"\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.279459Z","level":"error","event":"\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.281844Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.283175Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.283629Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.284389Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.295787Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.296608Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:38:42.297287Z","level":"error","event":"\t... 115 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:39:12.999865","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o245.json.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)\n\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\t\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)\n\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)\n\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)\n\t... 35 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\n\t... 50 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/main.py","lineno":22,"name":"run_etl_weather"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":27,"name":"extract_data_weather"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":468,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1362,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":282,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":327,"name":"get_return_value"}]}]}
{"timestamp":"2025-07-11T17:39:13.129207Z","level":"info","event":"Task instance in failure state","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.131869Z","level":"info","event":"Task start","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.132467Z","level":"info","event":"Task:<Task(PythonOperator): etl_data_weather.load>","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.133225Z","level":"info","event":"Failure caused by An error occurred while calling o245.json.","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.133755Z","level":"info","event":": org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.134732Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.135329Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.135916Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.136265Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.136729Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.137098Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.137538Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.137871Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.138224Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.138741Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.139258Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.139682Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.140050Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.140323Z","level":"info","event":"\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.140685Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.141264Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.142318Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.143830Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.144581Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.145035Z","level":"info","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.145399Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.145748Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.146044Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.146333Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.146754Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.147088Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.147392Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.147656Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.150459Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.151174Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.151665Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.152059Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.152385Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.152693Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.153130Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.153467Z","level":"info","event":"\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.153765Z","level":"info","event":"\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.154056Z","level":"info","event":"\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.154370Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.154671Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.154945Z","level":"info","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.155195Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.155467Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.155744Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.155991Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.156250Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.156520Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.156830Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.157087Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.157314Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.157593Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.157869Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.158118Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.158761Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.159173Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.160744Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.161271Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.161675Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.162034Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.162372Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.162747Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.163087Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.163910Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.164277Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.164623Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.164953Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.165342Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.165800Z","level":"info","event":"\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.166229Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.166598Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.167105Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.167457Z","level":"info","event":"\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.167741Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.168020Z","level":"info","event":"\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.168301Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.168566Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.168868Z","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.169138Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.169416Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.169672Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.170165Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.170460Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.170777Z","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.171069Z","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.171350Z","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.171640Z","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.171917Z","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.172168Z","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.172453Z","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.172729Z","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.173008Z","level":"info","event":"\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.173271Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.173553Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.173825Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.174086Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.174527Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.174856Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.175148Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.175475Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.175754Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.176049Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.176331Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.176626Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.176936Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.177201Z","level":"info","event":"\t\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.177685Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.178062Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.178324Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.178640Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.178908Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.179199Z","level":"info","event":"\t\tat scala.Option.getOrElse(Option.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.179651Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.179934Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.180204Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.180545Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.180789Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.181045Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.181341Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.181627Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.181877Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.182165Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.182486Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.183055Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.183393Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.183839Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.184350Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.184799Z","level":"info","event":"\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.185196Z","level":"info","event":"\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.185490Z","level":"info","event":"\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.185805Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.186091Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.186371Z","level":"info","event":"\t\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.186791Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.187094Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.187646Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.188048Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.188328Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.188652Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.189015Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.189333Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.189703Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.189988Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.190328Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.190602Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.190919Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.191267Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.191613Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.192186Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.192489Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.192829Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.193125Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.193411Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.193689Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.193966Z","level":"info","event":"\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.194272Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.194572Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.194846Z","level":"info","event":"\t\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.195088Z","level":"info","event":"\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.195375Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.195625Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.195879Z","level":"info","event":"\t\t... 22 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.196152Z","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.196492Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.196855Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.197134Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.197406Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.197822Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.198180Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.198837Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.200114Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.201325Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.202369Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.202818Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.203150Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.203542Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.203843Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.204125Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.204455Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.204750Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.205017Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.205346Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.205632Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.205902Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.206169Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.206420Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.206745Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.207007Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.207249Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.207628Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.208179Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.208738Z","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.209068Z","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.209398Z","level":"info","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.209669Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.209960Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.210234Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.210480Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.210762Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.211171Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.211462Z","level":"info","event":"Caused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.211748Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.212019Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.212289Z","level":"info","event":"\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.212548Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.212857Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.213114Z","level":"info","event":"\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.213385Z","level":"info","event":"\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.213760Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.214270Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.215162Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.215620Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.216471Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.218120Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.218961Z","level":"info","event":"\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.219341Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.220202Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.220656Z","level":"info","event":"\t... 35 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.220994Z","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.221329Z","level":"info","event":"\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.221987Z","level":"info","event":"\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.223369Z","level":"info","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.223889Z","level":"info","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.224272Z","level":"info","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.226511Z","level":"info","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.227076Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.227448Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.227739Z","level":"info","event":"\t... 50 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:39:13.227984Z","level":"info","event":"","chan":"stdout","logger":"task"}
