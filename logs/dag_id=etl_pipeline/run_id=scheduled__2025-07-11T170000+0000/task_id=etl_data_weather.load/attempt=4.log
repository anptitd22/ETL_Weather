{"timestamp":"2025-07-11T17:35:22.311495","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-11T17:35:22.312047","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:35:22.772739Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:24.423775Z","level":"error","event":"25/07/11 17:35:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:24.735664Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:24.736406Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:24.736844Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.345388Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.349908Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.356013Z","level":"info","event":"Current task name:etl_data_weather.load","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.360191Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.360787Z","level":"info","event":"extract data from file","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.361305Z","level":"info","event":"spark.app.id = local-1752255326369","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.363335Z","level":"info","event":"spark.hadoop.fs.s3a.connection.part.upload.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.364327Z","level":"info","event":"spark.hadoop.fs.s3a.socket.recv.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.364749Z","level":"info","event":"spark.hadoop.fs.s3a.impl.disable.cache = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.365109Z","level":"info","event":"spark.hadoop.fs.s3a.connection.request.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.365430Z","level":"info","event":"spark.app.initial.jar.urls = spark://8344c87913bd:35237/jars/ojdbc11.jar,spark://8344c87913bd:35237/jars/aws-java-sdk-bundle-1.12.787.jar,spark://8344c87913bd:35237/jars/hadoop-aws-3.3.6.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.366203Z","level":"info","event":"spark.hadoop.fs.s3a.path.style.access = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.366570Z","level":"info","event":"spark.rdd.compress = True","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.366983Z","level":"info","event":"spark.hadoop.fs.s3a.connection.idle.time = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.367489Z","level":"info","event":"spark.executor.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.369161Z","level":"info","event":"spark.driver.host = 8344c87913bd","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.369612Z","level":"info","event":"spark.hadoop.fs.s3a.endpoint = http://127.0.0.1:9000/","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.370004Z","level":"info","event":"spark.hadoop.fs.s3a.connection.establish.timeout = 5000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.370362Z","level":"info","event":"spark.hadoop.fs.s3a.access.key = admin","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.371080Z","level":"info","event":"spark.hadoop.fs.s3a.retry.interval = 500","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.371406Z","level":"info","event":"spark.jars = /opt/spark/jars/hadoop-aws-3.3.6.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,/opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.371687Z","level":"info","event":"spark.hadoop.fs.s3a.connection.maximum = 15","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.372505Z","level":"info","event":"spark.hadoop.fs.s3a.retry.limit = 5","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.373555Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2097152","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.373977Z","level":"info","event":"spark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.374492Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.min.seek.size = 131072","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.374982Z","level":"info","event":"spark.hadoop.fs.s3a.secret.key = admin12345","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.379207Z","level":"info","event":"spark.sql.adaptive.coalescePartitions.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.379751Z","level":"info","event":"spark.hadoop.fs.s3a.socket.send.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.380526Z","level":"info","event":"spark.hadoop.fs.s3a.fast.upload = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.380912Z","level":"info","event":"spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.381258Z","level":"info","event":"spark.sql.artifact.isolation.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.381541Z","level":"info","event":"spark.app.submitTime = 1752255324594","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.381896Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.size = 104857600","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.382235Z","level":"info","event":"spark.master = local[*]","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.382477Z","level":"info","event":"spark.driver.port = 35237","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.382728Z","level":"info","event":"spark.app.startTime = 1752255325069","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.382967Z","level":"info","event":"spark.executor.id = driver","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.383224Z","level":"info","event":"spark.driver.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.383472Z","level":"info","event":"spark.app.name = BAITEST","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.383765Z","level":"info","event":"spark.submit.deployMode = client","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.384095Z","level":"info","event":"spark.hadoop.fs.s3a.connection.ssl.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.384555Z","level":"info","event":"spark.serializer.objectStreamReset = 100","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.384826Z","level":"info","event":"spark.ui.showConsoleProgress = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.385093Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.purge.age = 86400","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.385380Z","level":"info","event":"spark.hadoop.fs.s3a.threads.max = 10","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.385665Z","level":"info","event":"spark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.385979Z","level":"info","event":"spark.submit.pyFiles =","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.386532Z","level":"info","event":"spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.387467Z","level":"info","event":"spark.hadoop.fs.s3a.attempts.maximum = 3","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.388216Z","level":"info","event":"spark.hadoop.fs.s3a.threads.keepalivetime = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.388718Z","level":"info","event":"spark.repl.local.jars = file:///opt/spark/jars/hadoop-aws-3.3.6.jar,file:///opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,file:///opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.389818Z","level":"info","event":"spark.sql.adaptive.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:38.390329Z","level":"info","event":"spark.hadoop.fs.s3a.connection.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.211091Z","level":"error","event":"25/07/11 17:35:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.503753Z","level":"error","event":"25/07/11 17:35:59 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.504471Z","level":"error","event":"You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.505160Z","level":"error","event":"This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.505616Z","level":"error","event":"The AWS SDK for Java 1.x is being used here:","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.506024Z","level":"error","event":"at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.506426Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.506750Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.507061Z","level":"error","event":"at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.507371Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1251)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.507693Z","level":"error","event":"at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.508053Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:982)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.508375Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.508664Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.508946Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.509233Z","level":"error","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.509524Z","level":"error","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.509787Z","level":"error","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.510035Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.510287Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.510514Z","level":"error","event":"at scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.510795Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.511200Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.511483Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.511835Z","level":"error","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.512188Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.512499Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.512736Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.512990Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.513284Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.513591Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.513894Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.514437Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.514725Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.515006Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.515286Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.515611Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.515897Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.516187Z","level":"error","event":"at scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.516450Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.516852Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.517149Z","level":"error","event":"at scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.517425Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.517709Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.517978Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.518247Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.518525Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.518780Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.519070Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.519413Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.519785Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.520055Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.520371Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.520753Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.521019Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.521381Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.521743Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.522143Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.522494Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.523025Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.523384Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.523713Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.524063Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.524356Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.524631Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.524877Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.525138Z","level":"error","event":"at scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.525409Z","level":"error","event":"at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.525672Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.525935Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.526213Z","level":"error","event":"at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.526458Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.526716Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.526993Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.527272Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.527576Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.527868Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.528159Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.528539Z","level":"error","event":"at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.528867Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.529153Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.529458Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.529759Z","level":"error","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.530060Z","level":"error","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.530408Z","level":"error","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.530802Z","level":"error","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.531132Z","level":"error","event":"at py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.531761Z","level":"error","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.532218Z","level":"error","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.532676Z","level":"error","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.533001Z","level":"error","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:35:59.533317Z","level":"error","event":"at java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.573610Z","level":"error","event":"25/07/11 17:36:29 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://weather-data/opt/airflow/dataset/weather_data.json.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.574361Z","level":"error","event":"org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.574841Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.575262Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.576948Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.577821Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.578246Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$25(S3AFileSystem.java:3629)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.578618Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.579062Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.579521Z","level":"error","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.579983Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.580352Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.581129Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3627)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.581782Z","level":"error","event":"\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.582404Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.583161Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.583959Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.586156Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.588453Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.589113Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.589520Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.589946Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.590325Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.590670Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.591024Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.591479Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.591836Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.592119Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.592452Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.592852Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.593346Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.593703Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.594071Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.594544Z","level":"error","event":"\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.594984Z","level":"error","event":"\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.595317Z","level":"error","event":"\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.595604Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.596173Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.597160Z","level":"error","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.597495Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.597809Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.598107Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.598636Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.598962Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.599294Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.599603Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.599901Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.600181Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.600462Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.600806Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.601147Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.601632Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.601924Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.602916Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.604664Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.605201Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.605872Z","level":"error","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.606235Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.606554Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.606893Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.607255Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.607571Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.607908Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.608180Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.608469Z","level":"error","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.609178Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.613178Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.614283Z","level":"error","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.615037Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.615453Z","level":"error","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.615822Z","level":"error","event":"\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.616419Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.616864Z","level":"error","event":"\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.617213Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.617531Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.617822Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.621896Z","level":"error","event":"\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.622576Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.623121Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.623511Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.623911Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.624358Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.624714Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.625071Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.628986Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.651546Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.652092Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.652499Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.652869Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.653354Z","level":"error","event":"Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.653737Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.654082Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.654445Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.654796Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.655087Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.655373Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.655636Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.655916Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.656186Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.656477Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.656760Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.657048Z","level":"error","event":"\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.657321Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.657588Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.657915Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.658192Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.658555Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.659590Z","level":"error","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.660005Z","level":"error","event":"\t... 84 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.660326Z","level":"error","event":"Caused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.660676Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.661226Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.661760Z","level":"error","event":"\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.662085Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.662394Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.662702Z","level":"error","event":"\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.663046Z","level":"error","event":"\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.663338Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.663613Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.663889Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.664223Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.664507Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.664807Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.665083Z","level":"error","event":"\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.665338Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.665618Z","level":"error","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.665889Z","level":"error","event":"\t... 100 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.666182Z","level":"error","event":"Caused by: java.net.ConnectException: Connection refused","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.666467Z","level":"error","event":"\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.666728Z","level":"error","event":"\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.666953Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.667315Z","level":"error","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.667607Z","level":"error","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.667883Z","level":"error","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.668260Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.668544Z","level":"error","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:36:29.668866Z","level":"error","event":"\t... 115 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.319862Z","level":"info","event":"Task instance in failure state","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.320512Z","level":"info","event":"Task start","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.320875Z","level":"info","event":"Task:<Task(PythonOperator): etl_data_weather.load>","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.321269Z","level":"info","event":"Failure caused by An error occurred while calling o245.json.","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.321623Z","level":"info","event":": org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.321957Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.322213Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.322466Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.322721Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.322953Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.323229Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.323514Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.323782Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.324030Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.324277Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.324539Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.324776Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.325022Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.325274Z","level":"info","event":"\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.325515Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.325806Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.326169Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.326453Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.326833Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.327090Z","level":"info","event":"\tat scala.Option.getOrElse(Option.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.327313Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.327542Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.327838Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.328105Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.328362Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.328617Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.328860Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.329107Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.329347Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.329595Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.329860Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.330063Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.330274Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.330565Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.331036Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.331428Z","level":"info","event":"\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.331713Z","level":"info","event":"\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.331952Z","level":"info","event":"\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.332171Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.332427Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.332696Z","level":"info","event":"\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.332963Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.333204Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.333465Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.333693Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.333892Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.334148Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.334370Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.334633Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.334895Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.335126Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.335348Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.335628Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.335872Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.336182Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.336487Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.336923Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.337403Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.337659Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.337902Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.338142Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.338379Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.338631Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.338992Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.339360Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.339719Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.339969Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.340210Z","level":"info","event":"\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.340427Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.340663Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.340908Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.341119Z","level":"info","event":"\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.341381Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.341611Z","level":"info","event":"\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.341819Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.342041Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.342274Z","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.342528Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.342804Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.343044Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.343427Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.343684Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.343903Z","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.344121Z","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.344533Z","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.344760Z","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.345020Z","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.345256Z","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.345513Z","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.345734Z","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.345972Z","level":"info","event":"\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.346214Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.346462Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.346689Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.346898Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.347173Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.347417Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.347660Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.347884Z","level":"info","event":"\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.348095Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.348306Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.348523Z","level":"info","event":"\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.348757Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.348982Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.349213Z","level":"info","event":"\t\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.349432Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.349681Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.349919Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.350174Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.350451Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.350738Z","level":"info","event":"\t\tat scala.Option.getOrElse(Option.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.350968Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.351218Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.351471Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.351689Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.351919Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.352131Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.352574Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.352985Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.353307Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.353584Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.353857Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.354255Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.354502Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.354785Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.355096Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.355388Z","level":"info","event":"\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.355657Z","level":"info","event":"\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.355918Z","level":"info","event":"\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.356174Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.356426Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.356793Z","level":"info","event":"\t\tat scala.collection.immutable.List.foreach(List.scala:334)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.357047Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.357299Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.357553Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.357796Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.358066Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.358332Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.358563Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.358822Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.359091Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.359321Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.359579Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.359829Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.360104Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.360340Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.360613Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.360879Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.361348Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.361634Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.361930Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.362217Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.362481Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.362758Z","level":"info","event":"\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.363002Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.363296Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.363558Z","level":"info","event":"\t\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.363805Z","level":"info","event":"\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.364068Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.364300Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.364624Z","level":"info","event":"\t\t... 22 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.364900Z","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.365158Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.365419Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.365673Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.365942Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.366187Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.204376","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o245.json.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://weather-data/opt/airflow/dataset/weather_data.json: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:217)\n\t\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:174)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3749)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\t\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\t\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\t\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\t\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 22 more\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1245)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1191)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:838)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:805)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:779)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)\n\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)\n\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)\n\t... 35 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n\tat java.base/java.net.Socket.connect(Socket.java:633)\n\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\n\t... 50 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/main.py","lineno":22,"name":"run_etl_weather"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":27,"name":"extract_data_weather"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":468,"name":"json"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1362,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":282,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":327,"name":"get_return_value"}]}]}
{"timestamp":"2025-07-11T17:37:06.367562Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:735)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.367809Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:717)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.368114Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:581)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.368422Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.368715Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5608)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.369019Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5555)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.369290Z","level":"info","event":"\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1423)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.369600Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$11(S3AFileSystem.java:2665)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.369911Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.370214Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.370502Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2653)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.370770Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2633)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.371050Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3724)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.371321Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3652)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.371593Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4636)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.371874Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.372135Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.372412Z","level":"info","event":"\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.372700Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.372970Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.373272Z","level":"info","event":"\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4634)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.373533Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.373816Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.374079Z","level":"info","event":"\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.374348Z","level":"info","event":"\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.374617Z","level":"info","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.374884Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.375181Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.375437Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.375717Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.375960Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.376210Z","level":"info","event":"\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.376467Z","level":"info","event":"Caused by: com.amazonaws.thirdparty.apache.http.conn.HttpHostConnectException: Connect to 127.0.0.1:9000 [/127.0.0.1] failed: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.376720Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.376976Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.377267Z","level":"info","event":"\tat jdk.internal.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.377546Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.377910Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.378204Z","level":"info","event":"\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.378488Z","level":"info","event":"\tat com.amazonaws.http.conn.$Proxy44.connect(Unknown Source)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.378889Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.379189Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.379462Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.379751Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.380311Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.380568Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.380856Z","level":"info","event":"\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.381196Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.381665Z","level":"info","event":"\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1183)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.383228Z","level":"info","event":"\t... 35 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.383745Z","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.384086Z","level":"info","event":"\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.384380Z","level":"info","event":"\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.384726Z","level":"info","event":"\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:547)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.385015Z","level":"info","event":"\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.385280Z","level":"info","event":"\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.385557Z","level":"info","event":"\tat java.base/java.net.Socket.connect(Socket.java:633)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.385827Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.386209Z","level":"info","event":"\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.386583Z","level":"info","event":"\t... 50 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-07-11T17:37:06.386901Z","level":"info","event":"","chan":"stdout","logger":"task"}
