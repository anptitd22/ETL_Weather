{"timestamp":"2025-07-11T12:02:33.319674","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:02:33.860724Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:02:36.250165Z","level":"error","event":"25/07/11 12:02:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:02:36.560744Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:02:36.561358Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:02:36.561807Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:13.348774","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:03:13.919409Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:15.680363Z","level":"error","event":"25/07/11 12:03:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:16.030866Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:16.041988Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:16.042538Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:53.833860","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:03:54.373440Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:56.175921Z","level":"error","event":"25/07/11 12:03:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:56.387338Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:56.403174Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:03:56.403744Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:04:36.353461","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:04:37.098560Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:04:38.965292Z","level":"error","event":"25/07/11 12:04:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:04:39.204366Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:04:39.218303Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:04:39.218685Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:16.799263","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:05:17.514317Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:18.880884Z","level":"error","event":"25/07/11 12:05:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:19.089226Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:19.118680Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:19.119120Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:55.924911","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:05:56.853793Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:58.882152Z","level":"error","event":"25/07/11 12:05:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:59.140651Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:59.141408Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:05:59.141850Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:06:36.199504","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:06:36.706884Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:06:38.647421Z","level":"error","event":"25/07/11 12:06:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:06:38.835017Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:06:38.843676Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:06:38.843974Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:15.695779","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:07:16.405942Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:18.395550Z","level":"error","event":"25/07/11 12:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:18.674096Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:18.685585Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:18.685947Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:33.882348","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:07:34.352512Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:35.861837Z","level":"error","event":"25/07/11 12:07:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:36.062133Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:36.071726Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:36.072107Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:56.220153","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:07:56.774531Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:58.555389Z","level":"error","event":"25/07/11 12:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:58.779598Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:58.785722Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:07:58.786209Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:08.596901","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:08:09.258654Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:11.444322Z","level":"error","event":"25/07/11 12:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:11.726732Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:11.727812Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:11.728402Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:49.113336","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:08:49.698774Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:51.979629Z","level":"error","event":"25/07/11 12:08:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:52.269454Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:52.287652Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:08:52.288157Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:07.986182","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:09:08.523551Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:10.819111Z","level":"error","event":"25/07/11 12:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:11.053417Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:11.061792Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:11.062273Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:12.320143Z","level":"error","event":"25/07/11 12:09:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:48.390086","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:09:49.000850Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:51.286472Z","level":"error","event":"25/07/11 12:09:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:51.573441Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:51.583008Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:51.583730Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:09:53.097105Z","level":"error","event":"25/07/11 12:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:29.618391","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:10:30.226185Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:31.825443Z","level":"error","event":"25/07/11 12:10:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:32.012049Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:32.019190Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:32.019714Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:10:33.083000Z","level":"error","event":"25/07/11 12:10:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:09.980712","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:11:10.713134Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:13.041228Z","level":"error","event":"25/07/11 12:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:13.343373Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:13.343982Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:13.344374Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:14.894119Z","level":"error","event":"25/07/11 12:11:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:51.599725","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:11:52.147217Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:54.403609Z","level":"error","event":"25/07/11 12:11:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:54.636273Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:54.643924Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:54.644408Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:11:55.803476Z","level":"error","event":"25/07/11 12:11:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:33.279246","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:12:34.072489Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:36.775857Z","level":"error","event":"25/07/11 12:12:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:37.153429Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:37.153943Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:37.154395Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:12:38.510124Z","level":"error","event":"25/07/11 12:12:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:16.163305","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:13:16.988777Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:19.395674Z","level":"error","event":"25/07/11 12:13:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:19.720787Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:19.721280Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:19.721661Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:21.157974Z","level":"error","event":"25/07/11 12:13:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:13:58.381311","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:13:58.927341Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:01.171215Z","level":"error","event":"25/07/11 12:14:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:01.471266Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:01.480929Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:01.481506Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:38.236463","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:14:38.756984Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:41.176099Z","level":"error","event":"25/07/11 12:14:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:41.492749Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:41.502873Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:14:41.503639Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:19.118750","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:15:19.665562Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:21.756260Z","level":"error","event":"25/07/11 12:15:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:21.989702Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:21.996914Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:21.997394Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:23.143081Z","level":"error","event":"25/07/11 12:15:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:23.150770Z","level":"error","event":"25/07/11 12:15:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:15:57.787004","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:15:58.354739Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:00.422623Z","level":"error","event":"25/07/11 12:16:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:00.675744Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:00.676272Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:00.676916Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:37.157052","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:16:37.712630Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:39.668557Z","level":"error","event":"25/07/11 12:16:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:39.918336Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:39.918639Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:16:39.918922Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:17.229312","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:17:17.891035Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:20.963017Z","level":"error","event":"25/07/11 12:17:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:21.380466Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:21.381642Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:21.382665Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:17:22.980065Z","level":"error","event":"25/07/11 12:17:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:00.402584","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:18:01.391602Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:03.932091Z","level":"error","event":"25/07/11 12:18:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:04.312667Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:04.332150Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:04.332831Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:41.778951","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:18:42.443923Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:44.698263Z","level":"error","event":"25/07/11 12:18:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:44.996820Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:44.997189Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:44.997535Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:18:46.015695Z","level":"error","event":"25/07/11 12:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:22.675371","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:19:23.428960Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:25.758519Z","level":"error","event":"25/07/11 12:19:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:26.078628Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:26.088359Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:26.088869Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:19:27.277448Z","level":"error","event":"25/07/11 12:19:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:03.780485","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:20:04.460875Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:06.534116Z","level":"error","event":"25/07/11 12:20:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:06.829161Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:06.829541Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:06.829857Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:08.016691Z","level":"error","event":"25/07/11 12:20:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:45.504889","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:20:46.102091Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:48.478573Z","level":"error","event":"25/07/11 12:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:48.770900Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:48.778484Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:48.778952Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:20:51.055405Z","level":"error","event":"25/07/11 12:20:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:26.519872","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:21:27.725562Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:30.044602Z","level":"error","event":"25/07/11 12:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:30.406357Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:30.420118Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:30.422352Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:21:31.837097Z","level":"error","event":"25/07/11 12:21:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:09.117762","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:22:10.008965Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:12.182135Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:12.190703Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:12.191201Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:12.594818Z","level":"error","event":"25/07/11 12:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:17.259126","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AttributeError","exc_value":"'SparkContext' object has no attribute 'hadoopConfiguration'","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":21,"name":"get_spark"}]}]}
{"timestamp":"2025-07-11T12:22:47.993602","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:22:48.562071Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:50.310362Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:50.320051Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:50.320601Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:50.764377Z","level":"error","event":"25/07/11 12:22:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:22:54.984917","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AttributeError","exc_value":"'SparkContext' object has no attribute 'hadoopConfiguration'","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":21,"name":"get_spark"}]}]}
{"timestamp":"2025-07-11T12:23:25.866570","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:23:26.444637Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:28.580963Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:28.588812Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:28.589416Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:28.984573Z","level":"error","event":"25/07/11 12:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:30.055275Z","level":"error","event":"25/07/11 12:23:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:23:33.907613","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AttributeError","exc_value":"'SparkContext' object has no attribute 'hadoopConfiguration'","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":21,"name":"get_spark"}]}]}
{"timestamp":"2025-07-11T12:24:05.356252","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:24:05.777000Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:07.645531Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:07.654617Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:07.655179Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:07.986492Z","level":"error","event":"25/07/11 12:24:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:09.090667Z","level":"error","event":"25/07/11 12:24:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:43.605092","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:24:44.233684Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:46.542453Z","level":"error","event":"25/07/11 12:24:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:46.768998Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:46.777164Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:24:46.777702Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:24.713464","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:25:25.256154Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:27.946624Z","level":"error","event":"25/07/11 12:25:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:28.300797Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:28.301282Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:28.301722Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:25:29.845593Z","level":"error","event":"25/07/11 12:25:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:08.688062","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:26:09.360540Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:12.037475Z","level":"error","event":"25/07/11 12:26:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:12.359953Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:12.370401Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:12.371061Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:13.732004Z","level":"error","event":"25/07/11 12:26:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:49.753604","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:26:50.395775Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:53.063179Z","level":"error","event":"25/07/11 12:26:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:53.305438Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:53.318918Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:26:53.320519Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:31.015630","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:27:31.649339Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:33.965188Z","level":"error","event":"25/07/11 12:27:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:34.251692Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:34.252297Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:34.252660Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:27:35.437976Z","level":"error","event":"25/07/11 12:27:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:11.414929","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:28:11.949079Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:14.065144Z","level":"error","event":"25/07/11 12:28:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:14.349445Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:14.349868Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:14.350139Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:51.767478","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:28:52.469617Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:55.560245Z","level":"error","event":"25/07/11 12:28:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:55.867518Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:55.891135Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:55.891726Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:28:57.149190Z","level":"error","event":"25/07/11 12:28:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:29:33.390799","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:29:34.166196Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:29:36.684222Z","level":"error","event":"25/07/11 12:29:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:29:37.017945Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:29:37.029219Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:29:37.029811Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:13.915925","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:30:14.962087Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:17.498959Z","level":"error","event":"25/07/11 12:30:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:17.829893Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:17.830202Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:17.830458Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:19.037694Z","level":"error","event":"25/07/11 12:30:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:55.443935","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:30:55.868001Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:57.610334Z","level":"error","event":"25/07/11 12:30:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:57.824848Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:57.825204Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:30:57.825521Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:33.927879","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:31:34.457275Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:36.498566Z","level":"error","event":"25/07/11 12:31:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:36.778879Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:36.779455Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:36.779826Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:31:37.948244Z","level":"error","event":"25/07/11 12:31:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:14.566669","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:32:15.228475Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:17.268776Z","level":"error","event":"25/07/11 12:32:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:17.501848Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:17.512091Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:17.512606Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:53.745612","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:32:54.365746Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:56.872314Z","level":"error","event":"25/07/11 12:32:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:57.115964Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:57.116355Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:57.116645Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:32:58.318071Z","level":"error","event":"25/07/11 12:32:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:34.584591","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:33:35.230112Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:37.468344Z","level":"error","event":"25/07/11 12:33:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:37.752440Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:37.752732Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:37.753032Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:33:38.858281Z","level":"error","event":"25/07/11 12:33:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:14.782338","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:34:15.356797Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:17.632332Z","level":"error","event":"25/07/11 12:34:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:17.957596Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:17.957905Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:17.958241Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:19.295971Z","level":"error","event":"25/07/11 12:34:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:34:57.134896","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:34:57.849278Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:00.798430Z","level":"error","event":"25/07/11 12:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:01.284310Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:01.303072Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:01.304072Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:40.533129","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:35:41.219894Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:43.787925Z","level":"error","event":"25/07/11 12:35:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:44.195038Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:44.205196Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:44.205849Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:35:45.814826Z","level":"error","event":"25/07/11 12:35:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:36:23.564575","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:36:24.228235Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:36:26.362296Z","level":"error","event":"25/07/11 12:36:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:36:26.634081Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:36:26.642463Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:36:26.643014Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:04.534671","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:37:05.103530Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:07.428698Z","level":"error","event":"25/07/11 12:37:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:07.693476Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:07.701720Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:07.702293Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:08.850131Z","level":"error","event":"25/07/11 12:37:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:45.029803","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:37:45.699534Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:47.981792Z","level":"error","event":"25/07/11 12:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:48.283613Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:48.284107Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:48.284390Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:37:49.685239Z","level":"error","event":"25/07/11 12:37:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:38:27.077587","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:38:27.828890Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:38:29.903059Z","level":"error","event":"25/07/11 12:38:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:38:30.195112Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:38:30.195652Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:38:30.195950Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:08.321945","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:39:09.983694Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:11.953321Z","level":"error","event":"25/07/11 12:39:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:12.249268Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:12.258327Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:12.258960Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:13.504045Z","level":"error","event":"25/07/11 12:39:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:49.374313","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:39:50.495956Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:52.992608Z","level":"error","event":"25/07/11 12:39:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:53.274213Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:53.283850Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:53.284468Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:39:54.781004Z","level":"error","event":"25/07/11 12:39:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:30.482022","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:40:31.574910Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:33.413603Z","level":"error","event":"25/07/11 12:40:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:33.684187Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:33.691243Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:33.691793Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:40:34.843022Z","level":"error","event":"25/07/11 12:40:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:10.977522","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:41:11.794785Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:14.103201Z","level":"error","event":"25/07/11 12:41:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:14.470116Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:14.478764Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:14.479357Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:51.294254","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:41:51.942850Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:54.407162Z","level":"error","event":"25/07/11 12:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:54.707417Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:54.717791Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:54.718319Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:41:56.111425Z","level":"error","event":"25/07/11 12:41:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:42:32.362260","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:42:33.012614Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:42:34.976655Z","level":"error","event":"25/07/11 12:42:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:42:35.170022Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:42:35.186092Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:42:35.186585Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:12.389915","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:43:12.957852Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:15.040033Z","level":"error","event":"25/07/11 12:43:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:15.293449Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:15.293888Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:15.294225Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:16.387366Z","level":"error","event":"25/07/11 12:43:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:52.619438","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:43:53.247065Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:55.810800Z","level":"error","event":"25/07/11 12:43:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:56.189353Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:56.198317Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:56.198833Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:43:57.953064Z","level":"error","event":"25/07/11 12:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:35.668720","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:44:36.306015Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:38.644016Z","level":"error","event":"25/07/11 12:44:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:38.972140Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:38.972704Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:38.973066Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:44:40.455558Z","level":"error","event":"25/07/11 12:44:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:17.350911","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:45:18.214058Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:21.013055Z","level":"error","event":"25/07/11 12:45:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:21.460626Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:21.461371Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:21.461856Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:22.979259Z","level":"error","event":"25/07/11 12:45:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:45:57.891595","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:45:58.511388Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:00.636167Z","level":"error","event":"25/07/11 12:46:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:00.893676Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:00.903405Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:00.903929Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:38.711938","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:46:39.372767Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:41.764980Z","level":"error","event":"25/07/11 12:46:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:42.119680Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:42.120430Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:42.120895Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:46:43.394132Z","level":"error","event":"25/07/11 12:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:20.059741","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:47:20.715374Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:23.008841Z","level":"error","event":"25/07/11 12:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:23.304904Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:23.314373Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:23.314891Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:47:24.561057Z","level":"error","event":"25/07/11 12:47:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:00.817114","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:48:01.413211Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:03.402481Z","level":"error","event":"25/07/11 12:48:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:03.614028Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:03.622797Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:03.623335Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:04.716352Z","level":"error","event":"25/07/11 12:48:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:40.384169","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:48:40.960882Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:42.999505Z","level":"error","event":"25/07/11 12:48:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:43.245653Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:43.252895Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:48:43.253389Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:19.487063","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:49:20.029261Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:22.111849Z","level":"error","event":"25/07/11 12:49:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:22.386889Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:22.395937Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:22.396516Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:49:59.382245","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:49:59.951258Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:01.974900Z","level":"error","event":"25/07/11 12:50:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:02.223895Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:02.231662Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:02.232113Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:41.520411","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:50:42.141697Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:44.113245Z","level":"error","event":"25/07/11 12:50:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:44.406591Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:44.406904Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:50:44.407233Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:51:22.828710","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:51:23.500745Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:51:25.664212Z","level":"error","event":"25/07/11 12:51:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:51:25.946743Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:51:25.947216Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:51:25.947631Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:02.465444","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:52:03.039360Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:04.946105Z","level":"error","event":"25/07/11 12:52:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:05.202603Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:05.202995Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:05.203320Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:41.092097","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:52:41.759669Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:44.789540Z","level":"error","event":"25/07/11 12:52:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:45.190847Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:45.213764Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:45.214385Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:52:46.584535Z","level":"error","event":"25/07/11 12:52:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:23.275301","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:53:23.879809Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:25.790200Z","level":"error","event":"25/07/11 12:53:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:26.054238Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:26.063259Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:26.063712Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:53:27.131246Z","level":"error","event":"25/07/11 12:53:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:04.310494","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:54:05.032059Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:07.235371Z","level":"error","event":"25/07/11 12:54:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:07.529136Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:07.537423Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:07.538039Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:08.780886Z","level":"error","event":"25/07/11 12:54:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:45.819559","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:54:46.641286Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:48.914935Z","level":"error","event":"25/07/11 12:54:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:49.205603Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:49.213785Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:49.214280Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:54:50.222185Z","level":"error","event":"25/07/11 12:54:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:26.645771","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:55:27.312484Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:29.801649Z","level":"error","event":"25/07/11 12:55:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:30.139520Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:30.139974Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:30.140324Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:55:31.435560Z","level":"error","event":"25/07/11 12:55:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:08.587287","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:56:09.245835Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:11.970549Z","level":"error","event":"25/07/11 12:56:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:12.385281Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:12.385770Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:12.386151Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:13.562686Z","level":"error","event":"25/07/11 12:56:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:48.587751","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:56:49.261708Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:51.264707Z","level":"error","event":"25/07/11 12:56:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:51.544785Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:51.562622Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:56:51.563061Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:29.741682","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:57:30.336323Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:32.748693Z","level":"error","event":"25/07/11 12:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:33.053163Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:33.053484Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:33.053754Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:57:34.697205Z","level":"error","event":"25/07/11 12:57:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:10.672661","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:58:11.609231Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:13.837930Z","level":"error","event":"25/07/11 12:58:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:14.082634Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:14.090981Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:14.091512Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:50.545368","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T12:58:51.676654Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:54.283864Z","level":"error","event":"25/07/11 12:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:54.617607Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:54.617996Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:54.618485Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T12:58:55.776052Z","level":"error","event":"25/07/11 12:58:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:29.135477","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T13:57:32.703198Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:35.461744Z","level":"error","event":"25/07/11 13:57:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:35.865689Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:35.873188Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:35.873953Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:57:37.840548Z","level":"error","event":"25/07/11 13:57:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:17.600739","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T13:58:18.119411Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:20.292776Z","level":"error","event":"25/07/11 13:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:20.582388Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:20.590616Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:20.591122Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:58:58.947405","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T13:58:59.836567Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:01.845768Z","level":"error","event":"25/07/11 13:59:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:02.144145Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:02.151409Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:02.151876Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:04.835819Z","level":"error","event":"25/07/11 13:59:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:40.985363","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T13:59:41.557174Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:43.420539Z","level":"error","event":"25/07/11 13:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:43.665122Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:43.673461Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T13:59:43.674073Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:21.796423","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:00:24.026360Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:27.245779Z","level":"error","event":"25/07/11 14:00:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:27.795711Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:27.803235Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:27.803791Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.416264Z","level":"error","event":"25/07/11 14:00:48 ERROR Inbox: Ignoring error","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.417143Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.418135Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.418593Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.419104Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.419529Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.420061Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.420592Z","level":"error","event":"\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.426008Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.427464Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.428720Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.430113Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.430674Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.431542Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.433659Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.434171Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.434574Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.434887Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.435296Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.435866Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.436250Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.437179Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.437754Z","level":"error","event":"Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@c873311233be:46401","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.438092Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.438393Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.438839Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.439107Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.439521Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.439970Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.440277Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.440864Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.441222Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.441618Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.441864Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.442114Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.442364Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.442691Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.442967Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.443377Z","level":"error","event":"\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.443720Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.444001Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.445679Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.446275Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.447250Z","level":"error","event":"\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.447650Z","level":"error","event":"\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.449241Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.449766Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.459276Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.460076Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.460788Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.462726Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.464535Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.465482Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.477214Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.478000Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.478429Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.478848Z","level":"error","event":"\tat scala.concurrent.Promise.complete(Promise.scala:57)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.480311Z","level":"error","event":"\tat scala.concurrent.Promise.complete$(Promise.scala:56)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.480722Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.481273Z","level":"error","event":"\tat scala.concurrent.Promise.success(Promise.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.481698Z","level":"error","event":"\tat scala.concurrent.Promise.success$(Promise.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.482933Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.483339Z","level":"error","event":"\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.483750Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.484074Z","level":"error","event":"\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.484402Z","level":"error","event":"\t... 8 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.484713Z","level":"error","event":"25/07/11 14:00:48 WARN Executor: Issue communicating with driver in heartbeater","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.485124Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.485595Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.485854Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.486149Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.486413Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.486678Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.486929Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.487919Z","level":"error","event":"\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.489331Z","level":"error","event":"\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.490433Z","level":"error","event":"\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.491305Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.491873Z","level":"error","event":"\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.492862Z","level":"error","event":"\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.493182Z","level":"error","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.495399Z","level":"error","event":"\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.495807Z","level":"error","event":"\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.496288Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.509456Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.510532Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.511018Z","level":"error","event":"Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.511837Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.512944Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.513733Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.515407Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.522884Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.525089Z","level":"error","event":"\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.525851Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.526295Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.526760Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.527084Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.538340Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.539077Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.539863Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.540276Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.545511Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.547753Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.548687Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.549116Z","level":"error","event":"\t... 3 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.555743Z","level":"error","event":"Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@c873311233be:46401","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.562892Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.563446Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.564490Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.566917Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.567718Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.568487Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.569189Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.571209Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.576909Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.579708Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.580658Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.581600Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.583934Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.584799Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.595810Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.604144Z","level":"error","event":"\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.604712Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.605101Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.605424Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.605802Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.608900Z","level":"error","event":"\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.609511Z","level":"error","event":"\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.609909Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.613090Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.615566Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.616901Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.619846Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.623223Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.630921Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.633442Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.635512Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.636872Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.645340Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.648000Z","level":"error","event":"\tat scala.concurrent.Promise.complete(Promise.scala:57)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.653176Z","level":"error","event":"\tat scala.concurrent.Promise.complete$(Promise.scala:56)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.654137Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.654485Z","level":"error","event":"\tat scala.concurrent.Promise.success(Promise.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.654747Z","level":"error","event":"\tat scala.concurrent.Promise.success$(Promise.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.655420Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.655735Z","level":"error","event":"\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.655988Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.720473Z","level":"error","event":"\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:48.721128Z","level":"error","event":"\t... 8 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:00:51.816729","level":"error","event":"Process timed out, PID: 1583","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T14:00:51.817274","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 1583","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":47,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T14:00:51.819588","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T14:01:23.304441","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:01:24.383830Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:01:26.593018Z","level":"error","event":"25/07/11 14:01:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:01:26.939664Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:01:26.948022Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:01:26.948492Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:04.567830","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:02:05.175171Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:07.825241Z","level":"error","event":"25/07/11 14:02:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:08.120602Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:08.130153Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:08.130661Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:46.213043","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:02:47.072817Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:49.683701Z","level":"error","event":"25/07/11 14:02:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:50.060564Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:50.082177Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:02:50.082738Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:29.970626","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:03:30.474335Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:32.604308Z","level":"error","event":"25/07/11 14:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:32.862266Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:32.862548Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:32.862789Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:03:34.030253Z","level":"error","event":"25/07/11 14:03:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:10.975084","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:04:11.750299Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:14.057773Z","level":"error","event":"25/07/11 14:04:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:14.380368Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:14.380909Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:14.381246Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:15.648258Z","level":"error","event":"25/07/11 14:04:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:51.441729","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:04:52.051096Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:54.529152Z","level":"error","event":"25/07/11 14:04:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:54.826921Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:54.827244Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:54.827533Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:04:56.061812Z","level":"error","event":"25/07/11 14:04:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:32.910136","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:05:33.548031Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:35.925145Z","level":"error","event":"25/07/11 14:05:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:36.216384Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:36.224611Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:36.225073Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:05:37.556813Z","level":"error","event":"25/07/11 14:05:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:15.006191","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:06:15.561619Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:17.656508Z","level":"error","event":"25/07/11 14:06:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:17.963843Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:17.964221Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:17.964519Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:56.562484","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:06:57.246430Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:59.496744Z","level":"error","event":"25/07/11 14:06:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:59.851445Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:59.859492Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:06:59.859964Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:36.669467","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:07:37.255316Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:38.978404Z","level":"error","event":"25/07/11 14:07:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:39.195900Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:39.203575Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:39.204026Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:56.090309","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:07:56.736946Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:58.491008Z","level":"error","event":"25/07/11 14:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:58.687762Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:58.697054Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:07:58.697479Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:08:34.672117","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:08:35.304057Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:08:37.439020Z","level":"error","event":"25/07/11 14:08:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:08:37.721117Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:08:37.730300Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:08:37.730666Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:13.620622","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:09:14.040078Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:15.359709Z","level":"error","event":"25/07/11 14:09:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:15.530570Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:15.539764Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:15.540086Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:51.571366","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:09:52.051327Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:53.913102Z","level":"error","event":"25/07/11 14:09:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:54.162369Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:54.177088Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:09:54.177583Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:10:29.908009","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:10:30.426942Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:10:32.654811Z","level":"error","event":"25/07/11 14:10:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:10:33.013579Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:10:33.026070Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:10:33.026547Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:09.448324","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:11:09.852900Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:11.192690Z","level":"error","event":"25/07/11 14:11:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:11.406686Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:11.424991Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:11.425643Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:47.072614","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:11:47.510938Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:48.955817Z","level":"error","event":"25/07/11 14:11:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:49.179785Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:49.189940Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:11:49.190267Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:12:25.184840","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:12:25.738104Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:12:28.014296Z","level":"error","event":"25/07/11 14:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:12:28.339826Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:12:28.357486Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:12:28.358013Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:05.155980","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:13:06.592215Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:08.759116Z","level":"error","event":"25/07/11 14:13:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:08.997766Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:09.008000Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:09.008426Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:45.851552","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:13:46.349647Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:48.188068Z","level":"error","event":"25/07/11 14:13:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:48.418891Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:48.419570Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:13:48.433160Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:14:25.467739","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:14:25.864041Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:14:27.252858Z","level":"error","event":"25/07/11 14:14:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:14:27.443760Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:14:27.454142Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:14:27.454619Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:02.652569","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:15:03.261740Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:04.998780Z","level":"error","event":"25/07/11 14:15:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:05.214080Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:05.214889Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:05.215362Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:41.073979","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:15:44.729256Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:46.038312Z","level":"error","event":"25/07/11 14:15:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:46.250464Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:46.260602Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:15:46.260966Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:16:21.742209","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:16:22.641461Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:16:24.482207Z","level":"error","event":"25/07/11 14:16:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:16:24.814184Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:16:24.825284Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:16:24.825746Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:01.308022","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:17:02.247637Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:04.452269Z","level":"error","event":"25/07/11 14:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:04.701000Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:04.715565Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:04.716135Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:40.860046","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:17:41.352787Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:43.195528Z","level":"error","event":"25/07/11 14:17:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:43.412451Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:43.421761Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:17:43.422177Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:19.405997","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:18:19.937613Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:21.429679Z","level":"error","event":"25/07/11 14:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:21.875847Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:21.894079Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:21.894702Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:57.457435","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:18:57.911275Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:59.268892Z","level":"error","event":"25/07/11 14:18:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:59.464058Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:59.471673Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:18:59.472115Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:19:35.302714","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:19:35.753482Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:19:37.138820Z","level":"error","event":"25/07/11 14:19:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:19:37.354163Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:19:37.370376Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:19:37.370825Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:13.152637","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:20:13.648376Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:15.236874Z","level":"error","event":"25/07/11 14:20:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:15.450396Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:15.464031Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:15.464542Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:51.645265","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:20:52.188132Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:53.850872Z","level":"error","event":"25/07/11 14:20:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:54.083646Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:54.095475Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:20:54.096004Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:21:30.001866","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:21:30.415977Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:21:32.164417Z","level":"error","event":"25/07/11 14:21:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:21:32.451348Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:21:32.451923Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:21:32.452251Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:07.925346","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:22:08.413118Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:10.644962Z","level":"error","event":"25/07/11 14:22:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:11.012171Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:11.013050Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:11.013397Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:49.546088","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:22:50.275017Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:52.511525Z","level":"error","event":"25/07/11 14:22:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:52.782178Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:52.795424Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:22:52.795970Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:23:30.051160","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:23:30.489008Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:23:32.060776Z","level":"error","event":"25/07/11 14:23:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:23:32.336450Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:23:32.348851Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:23:32.349334Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:08.088910","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:24:08.585168Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:10.636504Z","level":"error","event":"25/07/11 14:24:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:10.954589Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:10.955211Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:10.955533Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:47.802530","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:24:48.259284Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:49.762465Z","level":"error","event":"25/07/11 14:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:50.002757Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:50.013581Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:24:50.013937Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:25:26.106267","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:25:26.566806Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:25:27.993554Z","level":"error","event":"25/07/11 14:25:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:25:28.178333Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:25:28.187524Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:25:28.187909Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:03.726623","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:26:04.170605Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:05.897145Z","level":"error","event":"25/07/11 14:26:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:06.153840Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:06.165295Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:06.165843Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:42.033746","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:26:42.557710Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:44.748080Z","level":"error","event":"25/07/11 14:26:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:45.009774Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:45.021649Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:26:45.022181Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:20.770377","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:27:21.216409Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:23.193392Z","level":"error","event":"25/07/11 14:27:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:23.508195Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:23.518559Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:23.518930Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:27:59.618151","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:28:00.034092Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:01.618889Z","level":"error","event":"25/07/11 14:28:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:01.793282Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:01.802275Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:01.802725Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:38.126276","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T14:28:38.884813Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:40.193482Z","level":"error","event":"25/07/11 14:28:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:40.416071Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:40.416409Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T14:28:40.416622Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:10:14.913004","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:10:19.227700Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:10:22.410227Z","level":"error","event":"25/07/11 15:10:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:10:22.875255Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:10:22.889800Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:10:22.890460Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:04.782388","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:11:05.408427Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:08.421604Z","level":"error","event":"25/07/11 15:11:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:08.861355Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:08.862031Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:08.862355Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:10.632459Z","level":"error","event":"25/07/11 15:11:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:53.538519","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:11:55.368460Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:59.021188Z","level":"error","event":"25/07/11 15:11:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:59.503966Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:59.505303Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:11:59.506227Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:12:39.771846","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:12:40.256959Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:12:42.048904Z","level":"error","event":"25/07/11 15:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:12:42.357054Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:12:42.357407Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:12:42.357774Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:13:20.795015","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:13:21.520891Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:13:24.080700Z","level":"error","event":"25/07/11 15:13:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:13:24.446409Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:13:24.457596Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:13:24.458276Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:04.124094","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:14:05.304925Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:07.950363Z","level":"error","event":"25/07/11 15:14:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:08.255963Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:08.273109Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:08.273670Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:09.394341Z","level":"error","event":"25/07/11 15:14:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:45.643995","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:14:46.561970Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:48.659615Z","level":"error","event":"25/07/11 15:14:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:48.980511Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:48.991156Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:14:48.991709Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:15:26.364469","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:15:27.700004Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:15:30.057373Z","level":"error","event":"25/07/11 15:15:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:15:30.487977Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:15:30.488754Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:15:30.489139Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:07.829515","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:16:08.450535Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:10.663411Z","level":"error","event":"25/07/11 15:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:10.948593Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:10.957575Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:10.958191Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:12.028369Z","level":"error","event":"25/07/11 15:16:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:49.714459","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:16:50.590572Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:52.899009Z","level":"error","event":"25/07/11 15:16:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:53.234378Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:53.243233Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:53.243683Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:16:54.425456Z","level":"error","event":"25/07/11 15:16:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:30.603232","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:17:31.214884Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:34.238485Z","level":"error","event":"25/07/11 15:17:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:34.634974Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:34.643778Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:34.644380Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:17:37.097524Z","level":"error","event":"25/07/11 15:17:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:16.487857","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:18:17.124905Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:19.375081Z","level":"error","event":"25/07/11 15:18:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:19.673790Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:19.674358Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:19.674850Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:20.939156Z","level":"error","event":"25/07/11 15:18:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:56.891064","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:18:57.472742Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:59.485490Z","level":"error","event":"25/07/11 15:18:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:59.752659Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:59.763238Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:18:59.763805Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:01.058002Z","level":"error","event":"25/07/11 15:19:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:37.283873","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:19:37.872999Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:40.015827Z","level":"error","event":"25/07/11 15:19:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:40.288077Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:40.297486Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:19:40.297979Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:17.693677","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:20:18.215092Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:20.190861Z","level":"error","event":"25/07/11 15:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:20.505703Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:20.514672Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:20.515203Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:56.519476","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:20:57.129464Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:59.381404Z","level":"error","event":"25/07/11 15:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:59.776383Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:59.776905Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:20:59.777245Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:21:00.994241Z","level":"error","event":"25/07/11 15:21:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:29:46.404736","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:29:50.490532Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:29:53.194255Z","level":"error","event":"25/07/11 15:29:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:29:53.665710Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:29:53.666039Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:29:53.666366Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:30:34.959724","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:30:35.592381Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:30:38.163963Z","level":"error","event":"25/07/11 15:30:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:30:38.448577Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:30:38.458194Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:30:38.458611Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:17.677678","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:31:18.574081Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:20.782717Z","level":"error","event":"25/07/11 15:31:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:21.119722Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:21.120170Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:21.120464Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:22.337802Z","level":"error","event":"25/07/11 15:31:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:31:59.815436","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:32:00.532878Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:02.450545Z","level":"error","event":"25/07/11 15:32:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:02.682043Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:02.690596Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:02.691108Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:03.790554Z","level":"error","event":"25/07/11 15:32:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:38.968914","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:32:41.384238Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:43.946258Z","level":"error","event":"25/07/11 15:32:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:44.271008Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:44.279212Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:44.279812Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:32:46.077182Z","level":"error","event":"25/07/11 15:32:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:23.830408","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:33:25.032098Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:27.367493Z","level":"error","event":"25/07/11 15:33:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:27.740614Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:27.741300Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:27.741741Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:33:29.425276Z","level":"error","event":"25/07/11 15:33:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:09.689128","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:34:10.634762Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:13.160755Z","level":"error","event":"25/07/11 15:34:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:13.554423Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:13.580533Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:13.581108Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:51.633873","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:34:52.498593Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:54.373195Z","level":"error","event":"25/07/11 15:34:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:54.613421Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:54.613833Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:34:54.614140Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:35:31.215960","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:35:31.925176Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:35:34.886166Z","level":"error","event":"25/07/11 15:35:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:35:35.324121Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:35:35.324673Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:35:35.324984Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:13.825865","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:36:14.902494Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:17.751533Z","level":"error","event":"25/07/11 15:36:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:18.112155Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:18.120933Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:18.121576Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:19.480402Z","level":"error","event":"25/07/11 15:36:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:36:57.380405","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:36:57.991323Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:00.429694Z","level":"error","event":"25/07/11 15:37:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:00.731679Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:00.732126Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:00.732401Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:40.155466","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:37:40.761941Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:42.715697Z","level":"error","event":"25/07/11 15:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:42.956262Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:42.956633Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:42.956934Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:37:44.154472Z","level":"error","event":"25/07/11 15:37:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:21.105024","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:38:21.757597Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:24.348966Z","level":"error","event":"25/07/11 15:38:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:24.693478Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:24.702378Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:24.703009Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:38:26.061050Z","level":"error","event":"25/07/11 15:38:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:02.391196","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:39:02.892877Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:04.873406Z","level":"error","event":"25/07/11 15:39:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:05.180263Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:05.190517Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:05.191150Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:06.595233Z","level":"error","event":"25/07/11 15:39:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:42.710896","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:39:43.152491Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:44.718703Z","level":"error","event":"25/07/11 15:39:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:44.998249Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:44.998597Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:39:44.998882Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:20.266587","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:40:20.677064Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:22.307386Z","level":"error","event":"25/07/11 15:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:22.474648Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:22.475150Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:22.475492Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:23.394861Z","level":"error","event":"25/07/11 15:40:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:57.060400","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:40:57.527617Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:59.315962Z","level":"error","event":"25/07/11 15:40:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:59.506693Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:59.514715Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:40:59.515217Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:41:33.886766","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:41:34.345462Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:41:36.222341Z","level":"error","event":"25/07/11 15:41:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:41:36.422854Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:41:36.429948Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:41:36.430390Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:11.347053","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:42:11.875586Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:13.604908Z","level":"error","event":"25/07/11 15:42:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:13.781481Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:13.789410Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:13.790005Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:49.356289","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:42:49.742977Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:51.427418Z","level":"error","event":"25/07/11 15:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:51.594793Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:51.604354Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:42:51.604865Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:27.316334","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:43:27.787999Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:29.616234Z","level":"error","event":"25/07/11 15:43:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:29.793635Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:29.801397Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:29.801900Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:43:30.816636Z","level":"error","event":"25/07/11 15:43:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:04.599980","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:44:05.041365Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:07.230200Z","level":"error","event":"25/07/11 15:44:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:07.678208Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:07.685957Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:07.686447Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:08.981058Z","level":"error","event":"25/07/11 15:44:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:44.847831","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:44:45.282641Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:47.379005Z","level":"error","event":"25/07/11 15:44:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:47.590987Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:47.602929Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:47.603485Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:44:48.959548Z","level":"error","event":"25/07/11 15:44:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:24.024039","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:45:24.767967Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:27.616139Z","level":"error","event":"25/07/11 15:45:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:27.858979Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:27.875641Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:27.876168Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:45:29.193288Z","level":"error","event":"25/07/11 15:45:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:37.989272","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:52:41.619761Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:44.897193Z","level":"error","event":"25/07/11 15:52:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:45.477020Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:45.477619Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:45.477925Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:52:47.532544Z","level":"error","event":"25/07/11 15:52:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:53:22.869679","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:53:23.429573Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:53:25.607021Z","level":"error","event":"25/07/11 15:53:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:53:25.836067Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:53:25.844636Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:53:25.845272Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:02.189870","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:54:02.936772Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:05.779801Z","level":"error","event":"25/07/11 15:54:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:06.010858Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:06.021475Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:06.022696Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:08.009361Z","level":"error","event":"25/07/11 15:54:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:43.214536","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:54:44.012309Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:46.216782Z","level":"error","event":"25/07/11 15:54:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:46.463224Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:46.473298Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:54:46.473892Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:21.815714","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:55:22.306627Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:24.123321Z","level":"error","event":"25/07/11 15:55:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:24.290713Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:24.291310Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:24.291852Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:25.504964Z","level":"error","event":"25/07/11 15:55:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:55:59.758586","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:56:00.522540Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:02.100943Z","level":"error","event":"25/07/11 15:56:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:02.332766Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:02.345064Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:02.345717Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:39.165478","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:56:40.197923Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:41.879448Z","level":"error","event":"25/07/11 15:56:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:42.070993Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:42.094532Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:56:42.095112Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:18.262328","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:57:18.879674Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:20.506445Z","level":"error","event":"25/07/11 15:57:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:20.680926Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:20.688560Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:20.689035Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:56.925533","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:57:57.469162Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:57:59.798596Z","level":"error","event":"25/07/11 15:57:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:00.079671Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:00.095549Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:00.096124Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:37.704993","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:58:38.246212Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:40.402117Z","level":"error","event":"25/07/11 15:58:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:40.634562Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:40.660028Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:58:40.661496Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:16.482616","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:59:17.158819Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:19.629537Z","level":"error","event":"25/07/11 15:59:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:19.845375Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:19.855650Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:19.856346Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:55.456726","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T15:59:56.003873Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:58.225038Z","level":"error","event":"25/07/11 15:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:58.441864Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:58.450710Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T15:59:58.451266Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:00:36.285869","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:00:36.697858Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:00:38.441406Z","level":"error","event":"25/07/11 16:00:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:00:38.615108Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:00:38.622418Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:00:38.622984Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:14.371494","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:01:15.282928Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:17.588251Z","level":"error","event":"25/07/11 16:01:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:17.805952Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:17.806288Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:17.806601Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:53.496698","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:01:54.050094Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:56.112356Z","level":"error","event":"25/07/11 16:01:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:56.337301Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:56.346130Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:56.346675Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:01:57.484914Z","level":"error","event":"25/07/11 16:01:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:32.086189","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:02:32.535659Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:34.211146Z","level":"error","event":"25/07/11 16:02:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:34.393544Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:34.401599Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:34.402142Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:02:35.353303Z","level":"error","event":"25/07/11 16:02:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:09.271154","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:03:09.706778Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:11.471687Z","level":"error","event":"25/07/11 16:03:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:11.649382Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:11.658030Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:11.658609Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:12.632679Z","level":"error","event":"25/07/11 16:03:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:47.777132","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:03:48.318636Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:50.118156Z","level":"error","event":"25/07/11 16:03:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:50.304502Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:50.304769Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:03:50.305056Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:07:54.372342","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:07:59.200843Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:03.672176Z","level":"error","event":"25/07/11 16:08:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:04.073967Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:04.074386Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:04.075032Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:44.603408","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:08:45.132467Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:47.445641Z","level":"error","event":"25/07/11 16:08:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:47.676777Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:47.686462Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:08:47.686993Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:09:25.340119","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:09:26.137320Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:09:29.205011Z","level":"error","event":"25/07/11 16:09:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:09:29.542048Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:09:29.553248Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:09:29.553873Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:07.419013","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:10:08.210882Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:09.939023Z","level":"error","event":"25/07/11 16:10:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:10.125245Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:10.133358Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:10.133929Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:11.783967Z","level":"error","event":"25/07/11 16:10:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:46.892571","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:10:47.304954Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:49.217969Z","level":"error","event":"25/07/11 16:10:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:49.417599Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:49.417943Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:10:49.418256Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:24.758600","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:11:27.177385Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:29.291042Z","level":"error","event":"25/07/11 16:11:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:29.484306Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:29.493944Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:29.494489Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:11:30.585706Z","level":"error","event":"25/07/11 16:11:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:04.960899","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:12:05.492978Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:07.705162Z","level":"error","event":"25/07/11 16:12:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:07.943621Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:07.944234Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:07.944644Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:45.989624","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:12:46.879883Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:48.573528Z","level":"error","event":"25/07/11 16:12:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:48.799335Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:48.809996Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:12:48.810602Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:13:24.207213","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:13:24.972825Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:13:26.856007Z","level":"error","event":"25/07/11 16:13:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:13:27.031431Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:13:27.039560Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:13:27.040085Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:02.566800","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:14:04.477567Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:10.253524Z","level":"error","event":"25/07/11 16:14:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:10.540605Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:10.563963Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:10.564524Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:12.558207Z","level":"error","event":"25/07/11 16:14:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:48.683069","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:14:49.172820Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:51.231278Z","level":"error","event":"25/07/11 16:14:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:51.484706Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:51.485061Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:51.485440Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:14:52.720910Z","level":"error","event":"25/07/11 16:14:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:28.546727","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:15:29.073195Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:31.421003Z","level":"error","event":"25/07/11 16:15:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:31.664120Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:31.674330Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:31.674979Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:15:33.143281Z","level":"error","event":"25/07/11 16:15:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:10.020718","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:16:10.731051Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.047249Z","level":"error","event":"25/07/11 16:16:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.476934Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.486740Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.487280Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/auth-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.488082Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/s3-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.488453Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/utils-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.488835Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/awscore-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.489302Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/sdk-core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.489621Z","level":"error","event":"25/07/11 16:16:14 WARN DependencyUtils: Local jar /opt/spark/jars/http-client-spi-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.855939Z","level":"error","event":"25/07/11 16:16:14 INFO SparkContext: Running Spark version 4.0.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.884345Z","level":"error","event":"25/07/11 16:16:14 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.885033Z","level":"error","event":"25/07/11 16:16:14 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.919700Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.928800Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.929533Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.930208Z","level":"error","event":"25/07/11 16:16:14 INFO SparkContext: Submitted application: BAITEST","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.974597Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.984521Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:14.985036Z","level":"error","event":"25/07/11 16:16:14 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.091844Z","level":"error","event":"25/07/11 16:16:15 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.104864Z","level":"error","event":"25/07/11 16:16:15 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.105731Z","level":"error","event":"25/07/11 16:16:15 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.107677Z","level":"error","event":"25/07/11 16:16:15 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.108130Z","level":"error","event":"25/07/11 16:16:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.543021Z","level":"error","event":"25/07/11 16:16:15 INFO Utils: Successfully started service 'sparkDriver' on port 32949.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.602826Z","level":"error","event":"25/07/11 16:16:15 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.626448Z","level":"error","event":"25/07/11 16:16:15 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.661889Z","level":"error","event":"25/07/11 16:16:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.662260Z","level":"error","event":"25/07/11 16:16:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.662633Z","level":"error","event":"25/07/11 16:16:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.718412Z","level":"error","event":"25/07/11 16:16:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6dcaa558-e134-44e0-99c3-bf24db833ab6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:15.765299Z","level":"error","event":"25/07/11 16:16:15 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.020280Z","level":"error","event":"25/07/11 16:16:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.152407Z","level":"error","event":"25/07/11 16:16:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.170484Z","level":"error","event":"25/07/11 16:16:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.256921Z","level":"error","event":"25/07/11 16:16:16 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.4.1.jar at spark://12e2ddd9b857:32949/jars/hadoop-aws-3.4.1.jar with timestamp 1752250574847","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.257407Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.257803Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.258134Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.258505Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.258856Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.259214Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.259555Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.259890Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.260167Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.260461Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.260716Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.260996Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.261286Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.261569Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.261818Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.262068Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.262335Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.262649Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.263030Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.263359Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.263651Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.263924Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.264176Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.264457Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.264740Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/auth-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.265181Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/auth-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.265473Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.265777Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.266069Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.266367Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.266731Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.267120Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.267547Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.267849Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.268123Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.268399Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.268674Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.268950Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.269256Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.269518Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.269872Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.270166Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.270438Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.270828Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.271113Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.271462Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.271778Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.272048Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.272303Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/s3-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.274752Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/s3-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.303998Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.304244Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.304511Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.304744Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.304973Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.305192Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.305427Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.305660Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.306027Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.306300Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.306705Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.306978Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.307222Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.307465Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.307704Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.307960Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.308218Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.308480Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.308752Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.309051Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.309319Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.309555Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.309790Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/utils-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.310032Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/utils-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.310255Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.310511Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.310763Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.310995Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.311237Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.311496Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.311790Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.312159Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.312444Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.312686Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.312944Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.313197Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.313456Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.313785Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.314030Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.314437Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.314867Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.315253Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.315586Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.315836Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.316099Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.316432Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.316702Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/awscore-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.316991Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/awscore-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.317241Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.317490Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.317709Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.317923Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.345874Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.346161Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.346452Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.346714Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.346974Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.347230Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.347498Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.347773Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.348041Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.348374Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.348644Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.348912Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.349168Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.349418Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.349668Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.349955Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.350212Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.350484Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.350741Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/sdk-core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.351009Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/sdk-core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.351383Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.351971Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.352262Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.352632Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.352943Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.353416Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.353851Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.354478Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.354793Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.355185Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.355499Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.355785Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.356060Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.356420Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.356744Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.357049Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.357301Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.357569Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.357864Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.358182Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.358471Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.358828Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.359090Z","level":"error","event":"25/07/11 16:16:16 ERROR SparkContext: Failed to add /opt/spark/jars/http-client-spi-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.359360Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/http-client-spi-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.359631Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.359918Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.360258Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.360700Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.361087Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.361389Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.361925Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.362301Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.362671Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.382388Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.382997Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.383367Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.384339Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.385307Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.386072Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.386424Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.386798Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.387266Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.387675Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.388074Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.388550Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.388875Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.389152Z","level":"error","event":"25/07/11 16:16:16 INFO SparkContext: Added JAR /opt/spark/jars/ojdbc11.jar at spark://12e2ddd9b857:32949/jars/ojdbc11.jar with timestamp 1752250574847","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.389756Z","level":"error","event":"25/07/11 16:16:16 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.390185Z","level":"error","event":"25/07/11 16:16:16 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.390496Z","level":"error","event":"25/07/11 16:16:16 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.390878Z","level":"error","event":"25/07/11 16:16:16 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.391188Z","level":"error","event":"25/07/11 16:16:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.538859Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Starting executor ID driver on host 12e2ddd9b857","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.549858Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.550251Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.563713Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/ojdbc11.jar,file:/opt/airflow/ojdbc11.jar'","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.573130Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@38c2edb4 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.589176Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Fetching spark://12e2ddd9b857:32949/jars/hadoop-aws-3.4.1.jar with timestamp 1752250574847","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.677926Z","level":"error","event":"25/07/11 16:16:16 INFO TransportClientFactory: Successfully created connection to 12e2ddd9b857/172.18.0.10:32949 after 44 ms (0 ms spent in bootstraps)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.701630Z","level":"error","event":"25/07/11 16:16:16 INFO Utils: Fetching spark://12e2ddd9b857:32949/jars/hadoop-aws-3.4.1.jar to /tmp/spark-8f818371-ff10-488e-bf73-df94bf4417e0/userFiles-aae81dba-4333-4be7-81e4-947d934ce56f/fetchFileTemp8964494634248748790.tmp","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.786127Z","level":"error","event":"25/07/11 16:16:16 INFO Executor: Adding file:/tmp/spark-8f818371-ff10-488e-bf73-df94bf4417e0/userFiles-aae81dba-4333-4be7-81e4-947d934ce56f/hadoop-aws-3.4.1.jar to class loader default","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.805883Z","level":"error","event":"25/07/11 16:16:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45163.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.806498Z","level":"error","event":"25/07/11 16:16:16 INFO NettyBlockTransferService: Server created on 12e2ddd9b857:45163","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.816869Z","level":"error","event":"25/07/11 16:16:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.840892Z","level":"error","event":"25/07/11 16:16:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12e2ddd9b857, 45163, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.850484Z","level":"error","event":"25/07/11 16:16:16 INFO BlockManagerMasterEndpoint: Registering block manager 12e2ddd9b857:45163 with 434.4 MiB RAM, BlockManagerId(driver, 12e2ddd9b857, 45163, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.861386Z","level":"error","event":"25/07/11 16:16:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12e2ddd9b857, 45163, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:16.862029Z","level":"error","event":"25/07/11 16:16:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12e2ddd9b857, 45163, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.323536Z","level":"error","event":"25/07/11 16:16:20 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.330415Z","level":"error","event":"25/07/11 16:16:20 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.339664Z","level":"error","event":"25/07/11 16:16:20 INFO SparkUI: Stopped Spark web UI at http://12e2ddd9b857:4041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.353613Z","level":"error","event":"25/07/11 16:16:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.387362Z","level":"error","event":"25/07/11 16:16:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.387957Z","level":"error","event":"25/07/11 16:16:20 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.388347Z","level":"error","event":"25/07/11 16:16:20 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.396415Z","level":"error","event":"25/07/11 16:16:20 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.397045Z","level":"error","event":"25/07/11 16:16:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.409142Z","level":"error","event":"25/07/11 16:16:20 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.417630Z","level":"error","event":"25/07/11 16:16:20 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.418217Z","level":"error","event":"25/07/11 16:16:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f818371-ff10-488e-bf73-df94bf4417e0/pyspark-b4e8138e-f2db-45d7-9d38-eacc090f0ec7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.427421Z","level":"error","event":"25/07/11 16:16:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f818371-ff10-488e-bf73-df94bf4417e0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:20.427863Z","level":"error","event":"25/07/11 16:16:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-80937c85-eb47-41f6-aa26-66751c00845b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:51.717906","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:16:52.183533Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.359499Z","level":"error","event":"25/07/11 16:16:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.605632Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.613347Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.613865Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/auth-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.614316Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/s3-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.614702Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/utils-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.615090Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/awscore-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.615390Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/sdk-core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.615652Z","level":"error","event":"25/07/11 16:16:54 WARN DependencyUtils: Local jar /opt/spark/jars/http-client-spi-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.824392Z","level":"error","event":"25/07/11 16:16:54 INFO SparkContext: Running Spark version 4.0.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.832960Z","level":"error","event":"25/07/11 16:16:54 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.833505Z","level":"error","event":"25/07/11 16:16:54 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.869657Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.878618Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.879210Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.879620Z","level":"error","event":"25/07/11 16:16:54 INFO SparkContext: Submitted application: BAITEST","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.909061Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.918853Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.919683Z","level":"error","event":"25/07/11 16:16:54 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:54.995569Z","level":"error","event":"25/07/11 16:16:54 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.006475Z","level":"error","event":"25/07/11 16:16:54 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.007076Z","level":"error","event":"25/07/11 16:16:54 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.007830Z","level":"error","event":"25/07/11 16:16:54 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.009790Z","level":"error","event":"25/07/11 16:16:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.401270Z","level":"error","event":"25/07/11 16:16:55 INFO Utils: Successfully started service 'sparkDriver' on port 43107.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.442700Z","level":"error","event":"25/07/11 16:16:55 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.465163Z","level":"error","event":"25/07/11 16:16:55 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.481451Z","level":"error","event":"25/07/11 16:16:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.488725Z","level":"error","event":"25/07/11 16:16:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.489260Z","level":"error","event":"25/07/11 16:16:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.513425Z","level":"error","event":"25/07/11 16:16:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-71ecd9ad-3fc2-4039-8acf-ff5fc0820972","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.542015Z","level":"error","event":"25/07/11 16:16:55 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.777727Z","level":"error","event":"25/07/11 16:16:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.903288Z","level":"error","event":"25/07/11 16:16:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.967808Z","level":"error","event":"25/07/11 16:16:55 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.4.1.jar at spark://12e2ddd9b857:43107/jars/hadoop-aws-3.4.1.jar with timestamp 1752250614818","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.975831Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.976517Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.977025Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.977433Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.977826Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.978167Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.978512Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.978820Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.979092Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.979378Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.979732Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.980004Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.980267Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.980520Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.980808Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.981087Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.981338Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.981602Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.981870Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.982127Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.982343Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.982602Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.982829Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.983120Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.983549Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/auth-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.983843Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/auth-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.984244Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.984598Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.984905Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.985208Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.985521Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.985832Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.986116Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.986415Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.986708Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.986995Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.987265Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.987550Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.987811Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.988071Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.988373Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.988646Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.988908Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.989164Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.989422Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.989683Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.989940Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.990179Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.990416Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/s3-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.990662Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/s3-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.990906Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:55.991215Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.000305Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.002330Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.003532Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.004270Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.004700Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.005131Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.005459Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.005856Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.006180Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.006534Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.006999Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.007474Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.007765Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.008035Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.008460Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.008800Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.009128Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.009876Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.010195Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.010557Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.010864Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/utils-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.011205Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/utils-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.011521Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.011835Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.012141Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.012426Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.012730Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.013003Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.013280Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.013540Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.013842Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.014125Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.014410Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.014675Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.014954Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.015260Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.015527Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.015762Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.016002Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.016264Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.016502Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.016760Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.017056Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.017330Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.017888Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/awscore-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.018179Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/awscore-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.018452Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.018740Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.019030Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.019375Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.019670Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.019933Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.020175Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.043610Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.044189Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.044584Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.045170Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.045767Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.046122Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.046470Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.046837Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.047129Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.047513Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.047910Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.048261Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.048532Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.048818Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.049064Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.049359Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/sdk-core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.049676Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/sdk-core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.050098Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.050573Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.050872Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.051318Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.052033Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.052365Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.052682Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.052959Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.053212Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.053452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.053694Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.053963Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.054192Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.054459Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.054765Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.055002Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.055234Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.055467Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.055699Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.055929Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.056199Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.056561Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.056805Z","level":"error","event":"25/07/11 16:16:55 ERROR SparkContext: Failed to add /opt/spark/jars/http-client-spi-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.057036Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/http-client-spi-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.057261Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.057485Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.057710Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.057948Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.058184Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.058445Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.058721Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.058931Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.059183Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.059518Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.082232Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.082780Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.083146Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.083510Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.083899Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.084280Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.084643Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.084929Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.085212Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.085572Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.085857Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.086118Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.086373Z","level":"error","event":"25/07/11 16:16:55 INFO SparkContext: Added JAR /opt/spark/jars/ojdbc11.jar at spark://12e2ddd9b857:43107/jars/ojdbc11.jar with timestamp 1752250614818","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.086644Z","level":"error","event":"25/07/11 16:16:56 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.086908Z","level":"error","event":"25/07/11 16:16:56 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.087144Z","level":"error","event":"25/07/11 16:16:56 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.087388Z","level":"error","event":"25/07/11 16:16:56 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.087661Z","level":"error","event":"25/07/11 16:16:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.192600Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Starting executor ID driver on host 12e2ddd9b857","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.201061Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.201642Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.215654Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/ojdbc11.jar,file:/opt/airflow/ojdbc11.jar'","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.224518Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@79b7aaba for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.242698Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Fetching spark://12e2ddd9b857:43107/jars/hadoop-aws-3.4.1.jar with timestamp 1752250614818","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.304877Z","level":"error","event":"25/07/11 16:16:56 INFO TransportClientFactory: Successfully created connection to 12e2ddd9b857/172.18.0.10:43107 after 33 ms (0 ms spent in bootstraps)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.315616Z","level":"error","event":"25/07/11 16:16:56 INFO Utils: Fetching spark://12e2ddd9b857:43107/jars/hadoop-aws-3.4.1.jar to /tmp/spark-7ec70321-e562-487f-95e2-883011a519af/userFiles-834dffab-8ea2-47ce-9649-6e11fbb48668/fetchFileTemp10342324708976339129.tmp","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.362664Z","level":"error","event":"25/07/11 16:16:56 INFO Executor: Adding file:/tmp/spark-7ec70321-e562-487f-95e2-883011a519af/userFiles-834dffab-8ea2-47ce-9649-6e11fbb48668/hadoop-aws-3.4.1.jar to class loader default","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.372129Z","level":"error","event":"25/07/11 16:16:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40411.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.372731Z","level":"error","event":"25/07/11 16:16:56 INFO NettyBlockTransferService: Server created on 12e2ddd9b857:40411","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.381892Z","level":"error","event":"25/07/11 16:16:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.394367Z","level":"error","event":"25/07/11 16:16:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12e2ddd9b857, 40411, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.402384Z","level":"error","event":"25/07/11 16:16:56 INFO BlockManagerMasterEndpoint: Registering block manager 12e2ddd9b857:40411 with 434.4 MiB RAM, BlockManagerId(driver, 12e2ddd9b857, 40411, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.411783Z","level":"error","event":"25/07/11 16:16:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12e2ddd9b857, 40411, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:56.412306Z","level":"error","event":"25/07/11 16:16:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12e2ddd9b857, 40411, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.504114Z","level":"error","event":"25/07/11 16:16:59 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.513183Z","level":"error","event":"25/07/11 16:16:59 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.526894Z","level":"error","event":"25/07/11 16:16:59 INFO SparkUI: Stopped Spark web UI at http://12e2ddd9b857:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.544581Z","level":"error","event":"25/07/11 16:16:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.576219Z","level":"error","event":"25/07/11 16:16:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.593952Z","level":"error","event":"25/07/11 16:16:59 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.594553Z","level":"error","event":"25/07/11 16:16:59 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.595230Z","level":"error","event":"25/07/11 16:16:59 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.611607Z","level":"error","event":"25/07/11 16:16:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.622720Z","level":"error","event":"25/07/11 16:16:59 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.633845Z","level":"error","event":"25/07/11 16:16:59 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.634399Z","level":"error","event":"25/07/11 16:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec70321-e562-487f-95e2-883011a519af","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.642701Z","level":"error","event":"25/07/11 16:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-786c83b2-b530-4c18-b043-2a8404d93f92","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:16:59.650702Z","level":"error","event":"25/07/11 16:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec70321-e562-487f-95e2-883011a519af/pyspark-f7aad21c-1a50-44fa-a2fa-614dd2c4f2f4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:30.634480","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:17:31.107376Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:32.944854Z","level":"error","event":"25/07/11 16:17:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.138837Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.139159Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.147344Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/auth-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.147868Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/s3-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.148221Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/utils-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.148548Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/awscore-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.148866Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/sdk-core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.149135Z","level":"error","event":"25/07/11 16:17:33 WARN DependencyUtils: Local jar /opt/spark/jars/http-client-spi-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.406143Z","level":"error","event":"25/07/11 16:17:33 INFO SparkContext: Running Spark version 4.0.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.406713Z","level":"error","event":"25/07/11 16:17:33 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.407085Z","level":"error","event":"25/07/11 16:17:33 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.451293Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.452026Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.452727Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.453155Z","level":"error","event":"25/07/11 16:17:33 INFO SparkContext: Submitted application: BAITEST","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.487694Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.497171Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.497797Z","level":"error","event":"25/07/11 16:17:33 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.571229Z","level":"error","event":"25/07/11 16:17:33 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.571801Z","level":"error","event":"25/07/11 16:17:33 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.572183Z","level":"error","event":"25/07/11 16:17:33 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.572521Z","level":"error","event":"25/07/11 16:17:33 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.572860Z","level":"error","event":"25/07/11 16:17:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.860409Z","level":"error","event":"25/07/11 16:17:33 INFO Utils: Successfully started service 'sparkDriver' on port 42213.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.894682Z","level":"error","event":"25/07/11 16:17:33 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.913976Z","level":"error","event":"25/07/11 16:17:33 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.944198Z","level":"error","event":"25/07/11 16:17:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.944759Z","level":"error","event":"25/07/11 16:17:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.945216Z","level":"error","event":"25/07/11 16:17:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:33.966249Z","level":"error","event":"25/07/11 16:17:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42f5582d-6038-4a2f-b8d2-2c9f2a2bc4a1","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.007339Z","level":"error","event":"25/07/11 16:17:34 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.262162Z","level":"error","event":"25/07/11 16:17:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.378792Z","level":"error","event":"25/07/11 16:17:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.390203Z","level":"error","event":"25/07/11 16:17:34 INFO Utils: Successfully started service 'SparkUI' on port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.507869Z","level":"error","event":"25/07/11 16:17:34 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.4.1.jar at spark://12e2ddd9b857:42213/jars/hadoop-aws-3.4.1.jar with timestamp 1752250653391","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.508260Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.508940Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.510361Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.511446Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.512416Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.512806Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.513158Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.513476Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.514270Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.514799Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.515567Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.516261Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.517463Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.518475Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.519007Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.519406Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.519735Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.520029Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.520325Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.520705Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.520991Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.521408Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.521992Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.522316Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.522629Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/auth-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.523012Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/auth-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.523357Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.523729Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.524086Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.524369Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.524676Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.524941Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.525641Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.526056Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.526329Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.526892Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.527223Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.527582Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.527902Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.528195Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.528570Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.528878Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.529389Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.530053Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.530380Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.531175Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.531509Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.531886Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.532268Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/s3-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.532993Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/s3-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.575487Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.575778Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.576071Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.576362Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.576894Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.577189Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.577538Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.577812Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.578073Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.578363Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.578695Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.578982Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.579268Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.579519Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.579812Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.580100Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.580467Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.580762Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.581055Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.581337Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.581607Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.581997Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.582278Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/utils-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.582602Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/utils-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.582903Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.583180Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.583561Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.584030Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.584317Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.584601Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.584893Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.585174Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.585444Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.585714Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.586007Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.586285Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.586603Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.586879Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.587228Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.587543Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.587823Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.588121Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.588410Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.588706Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.589030Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.589314Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.589587Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/awscore-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.589881Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/awscore-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.590182Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.590465Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.590746Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.591047Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.607725Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.608035Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.608361Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.609003Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.609298Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.609712Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.610214Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.610507Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.610887Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.611906Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.612290Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.612600Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.612954Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.613251Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.613627Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.613948Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.614245Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.614570Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.614889Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/sdk-core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.615179Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/sdk-core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.615451Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.615761Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.616033Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.616367Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.616765Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.617244Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.617739Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.618163Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.618626Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.619215Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.619636Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.620068Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.620415Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.620731Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.621031Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.621335Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.621661Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.621980Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.622279Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.622584Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.622921Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.623203Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.623512Z","level":"error","event":"25/07/11 16:17:34 ERROR SparkContext: Failed to add /opt/spark/jars/http-client-spi-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.623793Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/http-client-spi-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.624099Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.624373Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.624715Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.625011Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.625528Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.625831Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.626132Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.626424Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.626785Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.636391Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.636983Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.637524Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.637934Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.638311Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.638653Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.638982Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.639318Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.639625Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.639923Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.640295Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.640590Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.640913Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.641256Z","level":"error","event":"25/07/11 16:17:34 INFO SparkContext: Added JAR /opt/spark/jars/ojdbc11.jar at spark://12e2ddd9b857:42213/jars/ojdbc11.jar with timestamp 1752250653391","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.641594Z","level":"error","event":"25/07/11 16:17:34 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.641889Z","level":"error","event":"25/07/11 16:17:34 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.642358Z","level":"error","event":"25/07/11 16:17:34 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.642681Z","level":"error","event":"25/07/11 16:17:34 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.643064Z","level":"error","event":"25/07/11 16:17:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.765521Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: Starting executor ID driver on host 12e2ddd9b857","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.777849Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.778711Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.794571Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/ojdbc11.jar,file:/opt/airflow/ojdbc11.jar'","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.804474Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6bd0d0eb for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.826460Z","level":"error","event":"25/07/11 16:17:34 INFO Executor: Fetching spark://12e2ddd9b857:42213/jars/hadoop-aws-3.4.1.jar with timestamp 1752250653391","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.939979Z","level":"error","event":"25/07/11 16:17:34 INFO TransportClientFactory: Successfully created connection to 12e2ddd9b857/172.18.0.10:42213 after 61 ms (0 ms spent in bootstraps)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:34.955492Z","level":"error","event":"25/07/11 16:17:34 INFO Utils: Fetching spark://12e2ddd9b857:42213/jars/hadoop-aws-3.4.1.jar to /tmp/spark-ed79ef33-4cad-4c86-9e94-fd929c14f6dd/userFiles-4a5a67b3-a1f3-4d65-aad3-206d05400c85/fetchFileTemp14930903074453165599.tmp","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.033301Z","level":"error","event":"25/07/11 16:17:35 INFO Executor: Adding file:/tmp/spark-ed79ef33-4cad-4c86-9e94-fd929c14f6dd/userFiles-4a5a67b3-a1f3-4d65-aad3-206d05400c85/hadoop-aws-3.4.1.jar to class loader default","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.042416Z","level":"error","event":"25/07/11 16:17:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45339.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.042968Z","level":"error","event":"25/07/11 16:17:35 INFO NettyBlockTransferService: Server created on 12e2ddd9b857:45339","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.050785Z","level":"error","event":"25/07/11 16:17:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.066664Z","level":"error","event":"25/07/11 16:17:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12e2ddd9b857, 45339, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.073415Z","level":"error","event":"25/07/11 16:17:35 INFO BlockManagerMasterEndpoint: Registering block manager 12e2ddd9b857:45339 with 434.4 MiB RAM, BlockManagerId(driver, 12e2ddd9b857, 45339, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.073998Z","level":"error","event":"25/07/11 16:17:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12e2ddd9b857, 45339, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:35.080359Z","level":"error","event":"25/07/11 16:17:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12e2ddd9b857, 45339, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:37.976596Z","level":"error","event":"25/07/11 16:17:37 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:37.977535Z","level":"error","event":"25/07/11 16:17:37 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:37.991731Z","level":"error","event":"25/07/11 16:17:37 INFO SparkUI: Stopped Spark web UI at http://12e2ddd9b857:4041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.017895Z","level":"error","event":"25/07/11 16:17:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.063825Z","level":"error","event":"25/07/11 16:17:38 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.070724Z","level":"error","event":"25/07/11 16:17:38 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.071187Z","level":"error","event":"25/07/11 16:17:38 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.080931Z","level":"error","event":"25/07/11 16:17:38 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.088751Z","level":"error","event":"25/07/11 16:17:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.118322Z","level":"error","event":"25/07/11 16:17:38 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.118717Z","level":"error","event":"25/07/11 16:17:38 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.119070Z","level":"error","event":"25/07/11 16:17:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b9b0828-9f62-42ef-a1df-441a446157c6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.128365Z","level":"error","event":"25/07/11 16:17:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed79ef33-4cad-4c86-9e94-fd929c14f6dd","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:17:38.128718Z","level":"error","event":"25/07/11 16:17:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed79ef33-4cad-4c86-9e94-fd929c14f6dd/pyspark-4dd67d9d-2d3c-4dcd-9440-05e2428e8526","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:08.982498","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:18:09.518093Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.267153Z","level":"error","event":"25/07/11 16:18:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.448037Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.456875Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.457163Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/auth-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.457465Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/s3-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.457722Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/utils-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.457955Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/awscore-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.458219Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/sdk-core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.458452Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/http-client-spi-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.458690Z","level":"error","event":"25/07/11 16:18:11 WARN DependencyUtils: Local jar /opt/spark/jars/credentials-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.637415Z","level":"error","event":"25/07/11 16:18:11 INFO SparkContext: Running Spark version 4.0.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.645438Z","level":"error","event":"25/07/11 16:18:11 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.645853Z","level":"error","event":"25/07/11 16:18:11 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.674939Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.682339Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.682775Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.683071Z","level":"error","event":"25/07/11 16:18:11 INFO SparkContext: Submitted application: BAITEST","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.709298Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.717522Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.717981Z","level":"error","event":"25/07/11 16:18:11 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.778919Z","level":"error","event":"25/07/11 16:18:11 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.788097Z","level":"error","event":"25/07/11 16:18:11 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.788353Z","level":"error","event":"25/07/11 16:18:11 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.788585Z","level":"error","event":"25/07/11 16:18:11 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:11.788793Z","level":"error","event":"25/07/11 16:18:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.105230Z","level":"error","event":"25/07/11 16:18:12 INFO Utils: Successfully started service 'sparkDriver' on port 42107.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.145592Z","level":"error","event":"25/07/11 16:18:12 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.161829Z","level":"error","event":"25/07/11 16:18:12 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.185457Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.185794Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.186020Z","level":"error","event":"25/07/11 16:18:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.207604Z","level":"error","event":"25/07/11 16:18:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-69278e07-979e-4b62-8e79-b6cd2b23e5cf","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.235180Z","level":"error","event":"25/07/11 16:18:12 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.406277Z","level":"error","event":"25/07/11 16:18:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.497463Z","level":"error","event":"25/07/11 16:18:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.548307Z","level":"error","event":"25/07/11 16:18:12 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.4.1.jar at spark://12e2ddd9b857:42107/jars/hadoop-aws-3.4.1.jar with timestamp 1752250691631","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.556330Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.556899Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.557290Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.557564Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.557808Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.558131Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.558430Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.558699Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.558960Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.559241Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.559592Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.559835Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.560081Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.560298Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.560567Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.560797Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.561028Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.561232Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.561444Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.561653Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.561976Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.562211Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.562455Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.562668Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.563029Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/auth-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.563390Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/auth-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.563785Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.564240Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.564611Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.564926Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.565244Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.565604Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.565857Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.566157Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.566694Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.566972Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.567232Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.567477Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.567738Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.567993Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.568251Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.568528Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.568762Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.568972Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.569205Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.569438Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.569653Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.569895Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.570091Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/s3-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.570292Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/s3-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.570516Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.570709Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.591146Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.591651Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.592044Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.592362Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.592634Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.592912Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.593205Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.593452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.593680Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.593912Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.594180Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.594526Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.595480Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.595798Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.596065Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.596306Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.596521Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.596791Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.597063Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.597305Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.597573Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/utils-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.597852Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/utils-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.598136Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.598398Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.598663Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.598965Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.599207Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.599477Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.599781Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.600040Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.600308Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.600531Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.600742Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.600954Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.601151Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.601384Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.601647Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.601874Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.602092Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.602309Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.602550Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.602791Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.603068Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.603362Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.603634Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/awscore-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.603871Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/awscore-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.604167Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.604402Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.604635Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.604848Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.605037Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.605259Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.605465Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.624743Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.625189Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.625534Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.625917Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.626425Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.626780Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.627121Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.627386Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.627613Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.627896Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.628211Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.628491Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.628871Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.629086Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.629274Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.629485Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/sdk-core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.629741Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/sdk-core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.629995Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.630214Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.630445Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.630675Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.630939Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.631159Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.631382Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.631632Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.631898Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.632130Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.632336Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.632554Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.632790Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.633044Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.633293Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.633520Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.633764Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.634003Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.634248Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.634522Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.634911Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.635144Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.635358Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/http-client-spi-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.635579Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/http-client-spi-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.635814Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.636009Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.636242Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.636443Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.636662Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.636884Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.637090Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.637281Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.637474Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.637661Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.657992Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.658505Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.658876Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.659167Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.659429Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.659718Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.659949Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.660180Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.660429Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.660718Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.660980Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.661193Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.661423Z","level":"error","event":"25/07/11 16:18:12 ERROR SparkContext: Failed to add /opt/spark/jars/credentials-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.661668Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/credentials-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.661911Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.662156Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.662368Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.662589Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.662880Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.663120Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.663384Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.663603Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.663845Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.664105Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.664337Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.664549Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.664758Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.664991Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.665233Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.665483Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.665716Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.666007Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.666247Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.666491Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.666790Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.667216Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.667457Z","level":"error","event":"25/07/11 16:18:12 INFO SparkContext: Added JAR /opt/spark/jars/ojdbc11.jar at spark://12e2ddd9b857:42107/jars/ojdbc11.jar with timestamp 1752250691631","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.667716Z","level":"error","event":"25/07/11 16:18:12 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.668051Z","level":"error","event":"25/07/11 16:18:12 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.668306Z","level":"error","event":"25/07/11 16:18:12 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.668541Z","level":"error","event":"25/07/11 16:18:12 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.668818Z","level":"error","event":"25/07/11 16:18:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.776752Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Starting executor ID driver on host 12e2ddd9b857","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.777035Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.777281Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.789194Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/ojdbc11.jar,file:/opt/airflow/ojdbc11.jar'","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.797356Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5b9df1c0 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.807074Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Fetching spark://12e2ddd9b857:42107/jars/hadoop-aws-3.4.1.jar with timestamp 1752250691631","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.873134Z","level":"error","event":"25/07/11 16:18:12 INFO TransportClientFactory: Successfully created connection to 12e2ddd9b857/172.18.0.10:42107 after 29 ms (0 ms spent in bootstraps)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.873542Z","level":"error","event":"25/07/11 16:18:12 INFO Utils: Fetching spark://12e2ddd9b857:42107/jars/hadoop-aws-3.4.1.jar to /tmp/spark-b70ab53b-4d66-4900-8d4b-383441c8b079/userFiles-1b7ff2ad-7abb-4071-9393-6bd7b1ae6869/fetchFileTemp17302906336287337593.tmp","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.918242Z","level":"error","event":"25/07/11 16:18:12 INFO Executor: Adding file:/tmp/spark-b70ab53b-4d66-4900-8d4b-383441c8b079/userFiles-1b7ff2ad-7abb-4071-9393-6bd7b1ae6869/hadoop-aws-3.4.1.jar to class loader default","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.926112Z","level":"error","event":"25/07/11 16:18:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44629.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.926683Z","level":"error","event":"25/07/11 16:18:12 INFO NettyBlockTransferService: Server created on 12e2ddd9b857:44629","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.934885Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.947575Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12e2ddd9b857, 44629, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.969055Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManagerMasterEndpoint: Registering block manager 12e2ddd9b857:44629 with 434.4 MiB RAM, BlockManagerId(driver, 12e2ddd9b857, 44629, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.969617Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12e2ddd9b857, 44629, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:12.970135Z","level":"error","event":"25/07/11 16:18:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12e2ddd9b857, 44629, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.166034Z","level":"error","event":"25/07/11 16:18:16 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.174752Z","level":"error","event":"25/07/11 16:18:16 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.191615Z","level":"error","event":"25/07/11 16:18:16 INFO SparkUI: Stopped Spark web UI at http://12e2ddd9b857:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.204294Z","level":"error","event":"25/07/11 16:18:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.220258Z","level":"error","event":"25/07/11 16:18:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.227072Z","level":"error","event":"25/07/11 16:18:16 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.227470Z","level":"error","event":"25/07/11 16:18:16 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.234903Z","level":"error","event":"25/07/11 16:18:16 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.235321Z","level":"error","event":"25/07/11 16:18:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.243712Z","level":"error","event":"25/07/11 16:18:16 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.249779Z","level":"error","event":"25/07/11 16:18:16 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.250055Z","level":"error","event":"25/07/11 16:18:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-2df8aa28-0b8b-41da-9085-589083de3629","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.255323Z","level":"error","event":"25/07/11 16:18:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b70ab53b-4d66-4900-8d4b-383441c8b079","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:16.260257Z","level":"error","event":"25/07/11 16:18:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b70ab53b-4d66-4900-8d4b-383441c8b079/pyspark-44b322b4-0114-49a0-b8e0-296fe9796cdb","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:47.261589","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:18:47.815607Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:49.959684Z","level":"error","event":"25/07/11 16:18:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.184011Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.216444Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.219959Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/auth-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.220655Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/s3-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.221352Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/utils-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.221725Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/awscore-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.222095Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/sdk-core-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.222445Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/http-client-spi-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.222939Z","level":"error","event":"25/07/11 16:18:50 WARN DependencyUtils: Local jar /opt/spark/jars/credentials-2.20.158.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.496832Z","level":"error","event":"25/07/11 16:18:50 INFO SparkContext: Running Spark version 4.0.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.506116Z","level":"error","event":"25/07/11 16:18:50 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.506741Z","level":"error","event":"25/07/11 16:18:50 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.552741Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.561865Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.562545Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.562970Z","level":"error","event":"25/07/11 16:18:50 INFO SparkContext: Submitted application: BAITEST","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.594050Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.602313Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.602943Z","level":"error","event":"25/07/11 16:18:50 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.671769Z","level":"error","event":"25/07/11 16:18:50 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.672103Z","level":"error","event":"25/07/11 16:18:50 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.672542Z","level":"error","event":"25/07/11 16:18:50 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.672979Z","level":"error","event":"25/07/11 16:18:50 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.673249Z","level":"error","event":"25/07/11 16:18:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:50.961567Z","level":"error","event":"25/07/11 16:18:50 INFO Utils: Successfully started service 'sparkDriver' on port 39897.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.001847Z","level":"error","event":"25/07/11 16:18:51 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.016968Z","level":"error","event":"25/07/11 16:18:51 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.037575Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.038153Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.044489Z","level":"error","event":"25/07/11 16:18:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.067009Z","level":"error","event":"25/07/11 16:18:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e3b496c0-7473-4ede-aaf8-ac2b75a666f3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.117006Z","level":"error","event":"25/07/11 16:18:51 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.280515Z","level":"error","event":"25/07/11 16:18:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.358889Z","level":"error","event":"25/07/11 16:18:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.370519Z","level":"error","event":"25/07/11 16:18:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.475033Z","level":"error","event":"25/07/11 16:18:51 INFO SparkContext: Added JAR /opt/spark/jars/hadoop-aws-3.4.1.jar at spark://12e2ddd9b857:39897/jars/hadoop-aws-3.4.1.jar with timestamp 1752250730491","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.475460Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.475738Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.476011Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.476275Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.476508Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.476764Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.476995Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.477245Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.477487Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.477724Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.477939Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.478150Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.478360Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.478582Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.478825Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.479053Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.479260Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.479485Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.479871Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.480103Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.480390Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.480609Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.480878Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.481133Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.481370Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/auth-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.481610Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/auth-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.481866Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.482121Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.482363Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.482565Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.482785Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.483037Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.483371Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.483645Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.483874Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.484098Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.484326Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.484555Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.484796Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.485050Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.485261Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.485529Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.485846Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.486129Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.486354Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.486593Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.486920Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.487290Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.487545Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/s3-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.487803Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/s3-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.514878Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.515142Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.515386Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.515674Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.515932Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.516175Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.516463Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.516747Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.517019Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.517369Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.517640Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.517921Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.518176Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.518501Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.518770Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.519033Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.519280Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.519539Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.519796Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.520046Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.520309Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.520573Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.520829Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/utils-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.521068Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/utils-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.521434Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.521671Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.521882Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.522120Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.522408Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.522686Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.522943Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.523233Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.523495Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.523776Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.524042Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.524305Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.524694Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.524960Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.525220Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.525502Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.525775Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.526038Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.526323Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.526667Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.526949Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.527205Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.527484Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/awscore-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.527754Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/awscore-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.527992Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.528267Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.528569Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.528877Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.537651Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.538289Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.538674Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.539013Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.539357Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.539742Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.540035Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.540309Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.540565Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.540833Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.541061Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.541312Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.541634Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.542060Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.542357Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.542637Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.542888Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.543175Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.543484Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/sdk-core-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.543897Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/sdk-core-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.544193Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.544462Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.544722Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.544987Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.545252Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.545531Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.545775Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.546031Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.546591Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.546890Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.547142Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.547533Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.547864Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.548199Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.548459Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.548722Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.548964Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.549210Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.549467Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.549700Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.549936Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.550210Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.550473Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/http-client-spi-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.550708Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/http-client-spi-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.550994Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.551281Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.551614Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.551906Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.552292Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.552735Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.553017Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.553448Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.553825Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.561480Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.562105Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.562499Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.562928Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.563361Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.563675Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.563957Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.564216Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.564488Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.564723Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.564963Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.565215Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.565465Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.565746Z","level":"error","event":"25/07/11 16:18:51 ERROR SparkContext: Failed to add /opt/spark/jars/credentials-2.20.158.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.565990Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/spark/jars/credentials-2.20.158.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.566229Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2174)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.566480Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2230)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.566709Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.566972Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.567235Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.567490Z","level":"error","event":"\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.567745Z","level":"error","event":"\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.568012Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:538)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.568372Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.568640Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.568898Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.569246Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.569543Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.569806Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.570110Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.570391Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.570705Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.570996Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.571278Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.571572Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.571826Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.572127Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.572405Z","level":"error","event":"25/07/11 16:18:51 INFO SparkContext: Added JAR /opt/spark/jars/ojdbc11.jar at spark://12e2ddd9b857:39897/jars/ojdbc11.jar with timestamp 1752250730491","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.572691Z","level":"error","event":"25/07/11 16:18:51 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.573003Z","level":"error","event":"25/07/11 16:18:51 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.573283Z","level":"error","event":"25/07/11 16:18:51 INFO SecurityManager: Changing view acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.573565Z","level":"error","event":"25/07/11 16:18:51 INFO SecurityManager: Changing modify acls groups to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.573837Z","level":"error","event":"25/07/11 16:18:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY; RPC SSL disabled","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.651067Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Starting executor ID driver on host 12e2ddd9b857","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.651677Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.652194Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.665859Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark/jars/ojdbc11.jar,file:/opt/airflow/ojdbc11.jar'","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.666515Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6bd0d0eb for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.687841Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Fetching spark://12e2ddd9b857:39897/jars/hadoop-aws-3.4.1.jar with timestamp 1752250730491","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.757313Z","level":"error","event":"25/07/11 16:18:51 INFO TransportClientFactory: Successfully created connection to 12e2ddd9b857/172.18.0.10:39897 after 35 ms (0 ms spent in bootstraps)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.765777Z","level":"error","event":"25/07/11 16:18:51 INFO Utils: Fetching spark://12e2ddd9b857:39897/jars/hadoop-aws-3.4.1.jar to /tmp/spark-0d307e65-a360-494a-9b88-b7f6c2692aa4/userFiles-0a3063a3-2c25-4f0c-b417-5188afa32a3f/fetchFileTemp2172451753493719089.tmp","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.817027Z","level":"error","event":"25/07/11 16:18:51 INFO Executor: Adding file:/tmp/spark-0d307e65-a360-494a-9b88-b7f6c2692aa4/userFiles-0a3063a3-2c25-4f0c-b417-5188afa32a3f/hadoop-aws-3.4.1.jar to class loader default","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.841173Z","level":"error","event":"25/07/11 16:18:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43725.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.841560Z","level":"error","event":"25/07/11 16:18:51 INFO NettyBlockTransferService: Server created on 12e2ddd9b857:43725","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.851101Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.868185Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 12e2ddd9b857, 43725, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.877525Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManagerMasterEndpoint: Registering block manager 12e2ddd9b857:43725 with 434.4 MiB RAM, BlockManagerId(driver, 12e2ddd9b857, 43725, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.885558Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 12e2ddd9b857, 43725, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:51.886157Z","level":"error","event":"25/07/11 16:18:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 12e2ddd9b857, 43725, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.480002Z","level":"error","event":"25/07/11 16:18:54 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.488159Z","level":"error","event":"25/07/11 16:18:54 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.503383Z","level":"error","event":"25/07/11 16:18:54 INFO SparkUI: Stopped Spark web UI at http://12e2ddd9b857:4041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.524978Z","level":"error","event":"25/07/11 16:18:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.554616Z","level":"error","event":"25/07/11 16:18:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.562347Z","level":"error","event":"25/07/11 16:18:54 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.562813Z","level":"error","event":"25/07/11 16:18:54 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.570240Z","level":"error","event":"25/07/11 16:18:54 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.579087Z","level":"error","event":"25/07/11 16:18:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.593437Z","level":"error","event":"25/07/11 16:18:54 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.594025Z","level":"error","event":"25/07/11 16:18:54 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.594359Z","level":"error","event":"25/07/11 16:18:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d307e65-a360-494a-9b88-b7f6c2692aa4/pyspark-51ec7edb-4109-4045-95bc-96db931b34fb","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.603579Z","level":"error","event":"25/07/11 16:18:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e100a7-a684-4c6e-a9df-3f7769f2c216","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:18:54.610357Z","level":"error","event":"25/07/11 16:18:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d307e65-a360-494a-9b88-b7f6c2692aa4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:34:59.297142","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:35:02.037235Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:04.215262Z","level":"error","event":"25/07/11 16:35:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:04.729752Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:04.740888Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:04.741846Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:43.051980","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:35:43.751362Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:45.910155Z","level":"error","event":"25/07/11 16:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:46.147679Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:46.156380Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:35:46.156793Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:22.966741","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:36:23.906301Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:26.546035Z","level":"error","event":"25/07/11 16:36:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:26.796987Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:26.804210Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:26.804682Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:36:28.032069Z","level":"error","event":"25/07/11 16:36:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:05.010772","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:37:06.223750Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:08.832829Z","level":"error","event":"25/07/11 16:37:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:09.129777Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:09.139060Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:09.139598Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:10.518533Z","level":"error","event":"25/07/11 16:37:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:49.331754","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:37:50.714876Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:53.298422Z","level":"error","event":"25/07/11 16:37:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:53.625759Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:53.633453Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:37:53.633975Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:38:32.664231","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:38:33.696821Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:38:35.567676Z","level":"error","event":"25/07/11 16:38:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:38:35.791923Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:38:35.798464Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:38:35.798863Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:15.810849","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T16:39:16.490492Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:19.319729Z","level":"error","event":"25/07/11 16:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:19.639286Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:19.649632Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:19.650131Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T16:39:21.493012Z","level":"error","event":"25/07/11 16:39:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:23.350820","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:20:27.129452Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:29.966137Z","level":"error","event":"25/07/11 17:20:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:30.372288Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:30.381524Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:30.382118Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:20:32.427829Z","level":"error","event":"25/07/11 17:20:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:11.376886","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:21:12.033262Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:14.742985Z","level":"error","event":"25/07/11 17:21:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:15.146895Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:15.147355Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:15.147720Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:16.569355Z","level":"error","event":"25/07/11 17:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:21:58.784447","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:22:01.719779Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:05.285967Z","level":"error","event":"25/07/11 17:22:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:05.730129Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:05.737293Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:05.738406Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:07.947294Z","level":"error","event":"25/07/11 17:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:53.431981","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:22:54.018906Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:56.760061Z","level":"error","event":"25/07/11 17:22:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:57.288006Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:57.288603Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:22:57.289044Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:23:42.349897","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:23:43.415895Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:23:46.255747Z","level":"error","event":"25/07/11 17:23:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:23:46.588571Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:23:46.595948Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:23:46.596401Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:30.902337","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:24:32.164056Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:34.720708Z","level":"error","event":"25/07/11 17:24:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:35.035320Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:35.035642Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:35.035947Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:24:36.476882Z","level":"error","event":"25/07/11 17:24:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:25:01.477288","level":"error","event":"Process timed out, PID: 1987","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:25:01.542025","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 1987","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":50,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:25:02.337245","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:25:34.818965","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:25:35.603676Z","level":"info","event":"[0.004s][warning][perf,memops] Cannot use file /tmp/hsperfdata_airflow/2396 because it is locked by another process (errno = 11)","chan":"stdout","logger":"processor"}
{"timestamp":"2025-07-11T17:25:35.826542Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:25:37.977086Z","level":"error","event":"25/07/11 17:25:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:25:38.256941Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:25:38.264995Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:25:38.265469Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:26:16.152432","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:26:16.810740Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:26:20.129938Z","level":"error","event":"25/07/11 17:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:26:20.638407Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:26:20.650319Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:26:20.650843Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:02.756256","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:27:03.872972Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:07.038851Z","level":"error","event":"25/07/11 17:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:07.601996Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:07.614754Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:07.615289Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:47.226919","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:27:47.844666Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:50.344676Z","level":"error","event":"25/07/11 17:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:50.634963Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:50.644495Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:50.644983Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:27:51.938683Z","level":"error","event":"25/07/11 17:27:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:28:29.978442","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:28:30.595247Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:28:32.961518Z","level":"error","event":"25/07/11 17:28:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:28:33.238703Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:28:33.239203Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:28:33.239541Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:10.419762","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:29:11.034007Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:13.172592Z","level":"error","event":"25/07/11 17:29:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:13.451549Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:13.461099Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:13.461596Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:51.236895","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:29:51.804435Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:54.005793Z","level":"error","event":"25/07/11 17:29:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:54.294083Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:54.305149Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:54.306002Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:29:55.439489Z","level":"error","event":"25/07/11 17:29:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:31.946110","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:30:33.201366Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:37.070056Z","level":"error","event":"25/07/11 17:30:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:37.464170Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:37.493260Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:37.493878Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:30:39.536496Z","level":"error","event":"25/07/11 17:30:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:02.038764","level":"error","event":"Process timed out, PID: 5078","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:31:02.279532","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 5078","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":50,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:31:02.564208","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:31:33.954701","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:31:34.834747Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:37.915235Z","level":"error","event":"25/07/11 17:31:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:38.248278Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:38.257782Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:38.258370Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:31:39.454965Z","level":"error","event":"25/07/11 17:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:15.934911","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:32:16.712405Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:19.473871Z","level":"error","event":"25/07/11 17:32:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:19.756776Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:19.772134Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:19.773691Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:32:58.686082","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:32:59.324672Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:01.658142Z","level":"error","event":"25/07/11 17:33:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:01.973688Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:01.982560Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:01.983118Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:39.738468","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:33:40.503260Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:43.829991Z","level":"error","event":"25/07/11 17:33:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:44.507174Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:44.528679Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:44.533728Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:33:46.882083Z","level":"error","event":"25/07/11 17:33:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:09.807436","level":"error","event":"Process timed out, PID: 6631","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:34:09.907188","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 6631","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:34:10.308900","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:34:42.837481","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:34:43.565849Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:45.990923Z","level":"error","event":"25/07/11 17:34:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:46.282016Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:46.294677Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:46.297267Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:34:47.850326Z","level":"error","event":"25/07/11 17:34:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:25.621993","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:35:26.266247Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:29.563423Z","level":"error","event":"25/07/11 17:35:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:29.563814Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:29.564119Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:29.564446Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:31.370573Z","level":"error","event":"25/07/11 17:35:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:35:55.843281","level":"error","event":"Process timed out, PID: 7409","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:35:56.067102","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 7409","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:35:56.584342","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:36:28.801096","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:36:29.784331Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:32.832517Z","level":"error","event":"25/07/11 17:36:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:33.262342Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:33.273657Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:33.274131Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:35.302356Z","level":"error","event":"25/07/11 17:36:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:36:59.012598","level":"error","event":"Process timed out, PID: 7804","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:36:59.961065","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 7804","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:37:02.482147","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:37:04.185338Z","level":"error","event":"25/07/11 17:37:04 ERROR SparkContext: Error initializing SparkContext.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.197563Z","level":"error","event":"java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.198477Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.add(ShutdownHookManager.scala:212)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.198874Z","level":"error","event":"\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.199198Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:701)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.199493Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.199755Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.200007Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.200255Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.200491Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.200808Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.201088Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.201330Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.201573Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.202367Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.202687Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.207117Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.207669Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:04.208305Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:35.640706","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:37:36.514719Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:40.207600Z","level":"error","event":"25/07/11 17:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:40.764147Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:40.764733Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:37:40.765232Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:38:21.496189","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:38:22.154860Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:38:24.390662Z","level":"error","event":"25/07/11 17:38:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:38:24.729108Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:38:24.729497Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:38:24.729936Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:03.119399","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:39:03.895272Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:07.289790Z","level":"error","event":"25/07/11 17:39:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:07.762602Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:07.763414Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:07.763914Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:09.324776Z","level":"error","event":"25/07/11 17:39:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:47.600176","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:39:48.348594Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:50.757793Z","level":"error","event":"25/07/11 17:39:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:51.044020Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:51.057407Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:51.057978Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:39:53.146770Z","level":"error","event":"25/07/11 17:39:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:40:31.131061","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:40:31.909423Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:40:34.228555Z","level":"error","event":"25/07/11 17:40:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:40:34.570686Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:40:34.592773Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:40:34.594438Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:13.712329","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:41:14.510898Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:17.469277Z","level":"error","event":"25/07/11 17:41:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:17.797870Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:17.807890Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:17.808468Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:41:57.406194","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:41:58.175295Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:42:01.598162Z","level":"error","event":"25/07/11 17:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:42:02.021241Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:42:02.029169Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:42:02.029636Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:42:27.405990","level":"error","event":"Process timed out, PID: 10438","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T17:42:27.406967","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 10438","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T17:42:27.420797","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T17:42:59.061681","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:42:59.845136Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:02.377728Z","level":"error","event":"25/07/11 17:43:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:02.695343Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:02.705455Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:02.705997Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:43.085481","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:43:43.873632Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:46.407795Z","level":"error","event":"25/07/11 17:43:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:46.819965Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:46.831898Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:43:46.832482Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:44:29.393115","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:44:30.568933Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:44:33.285476Z","level":"error","event":"25/07/11 17:44:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:44:33.621548Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:44:33.631228Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:44:33.631786Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:12.027136","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:45:12.984303Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:15.626360Z","level":"error","event":"25/07/11 17:45:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:16.176880Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:16.177214Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:16.177524Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:55.650704","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:45:56.593591Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:58.788207Z","level":"error","event":"25/07/11 17:45:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:59.067434Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:59.076135Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:45:59.076841Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:00.247951Z","level":"error","event":"25/07/11 17:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:38.385271","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:46:39.097388Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:41.758593Z","level":"error","event":"25/07/11 17:46:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:42.077503Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:42.086691Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:46:42.087260Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:24.340705","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:49:27.185317Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:35.153321Z","level":"error","event":"25/07/11 17:49:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:35.848019Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:35.858166Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:35.858785Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:49:38.572348Z","level":"error","event":"25/07/11 17:49:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:50:20.601460","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:50:21.396485Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:50:24.128835Z","level":"error","event":"25/07/11 17:50:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:50:24.486923Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:50:24.516714Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:50:24.517261Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:03.040088","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:51:04.425104Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:07.601494Z","level":"error","event":"25/07/11 17:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:08.157640Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:08.184286Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:08.192426Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:10.104458Z","level":"error","event":"25/07/11 17:51:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:50.102320","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:51:50.730620Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:52.469080Z","level":"error","event":"25/07/11 17:51:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:52.712566Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:52.720403Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:52.720950Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:51:54.173320Z","level":"error","event":"25/07/11 17:51:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:52:30.463397","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:52:30.939313Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:52:32.911090Z","level":"error","event":"25/07/11 17:52:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:52:33.343372Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:52:33.353895Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:52:33.354415Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:10.881365","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:53:11.406801Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:13.800672Z","level":"error","event":"25/07/11 17:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:14.280568Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:14.285477Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:14.286179Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:53.928791","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:53:54.325120Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:55.890692Z","level":"error","event":"25/07/11 17:53:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:56.163761Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:56.170938Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:53:56.171487Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:32.670961","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:54:33.116785Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:35.268132Z","level":"error","event":"25/07/11 17:54:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:35.505580Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:35.513240Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:35.513696Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:54:36.978519Z","level":"error","event":"25/07/11 17:54:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:13.120751","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:55:13.749409Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:15.252346Z","level":"error","event":"25/07/11 17:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:15.473411Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:15.480139Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:15.480611Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:54.666231","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:55:56.154099Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:59.510221Z","level":"error","event":"25/07/11 17:55:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:59.893293Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:59.902568Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:55:59.903323Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:56:38.504771","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:56:39.433734Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:56:41.094525Z","level":"error","event":"25/07/11 17:56:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:56:41.387549Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:56:41.397410Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:56:41.397913Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:57:19.652978","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:57:20.150501Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:57:22.013376Z","level":"error","event":"25/07/11 17:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:57:22.328368Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:57:22.340305Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:57:22.341021Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:00.061178","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:58:00.489195Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:02.203191Z","level":"error","event":"25/07/11 17:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:02.450106Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:02.456295Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:02.456748Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:03.778999Z","level":"error","event":"25/07/11 17:58:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:40.952215","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:58:41.705633Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:43.891014Z","level":"error","event":"25/07/11 17:58:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:44.317637Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:44.330189Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:58:44.330825Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:21.601137","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T17:59:22.270324Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:25.230411Z","level":"error","event":"25/07/11 17:59:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:25.539434Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:25.929000Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:25.929545Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T17:59:27.684815Z","level":"error","event":"25/07/11 17:59:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:07.464993","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:00:08.016054Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:10.494561Z","level":"error","event":"25/07/11 18:00:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:10.770464Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:10.778745Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:10.779406Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:52.152813","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:00:52.595554Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:54.576607Z","level":"error","event":"25/07/11 18:00:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:54.849117Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:54.856263Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:00:54.856776Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:32.777788","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:01:33.212565Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:35.311620Z","level":"error","event":"25/07/11 18:01:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:35.601540Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:35.611569Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:35.612353Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:01:37.017980Z","level":"error","event":"25/07/11 18:01:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:14.095039","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:02:14.646887Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:16.928159Z","level":"error","event":"25/07/11 18:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:17.149177Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:17.155584Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:17.156146Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:18.106807Z","level":"error","event":"25/07/11 18:02:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:54.161328","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:02:54.746022Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:57.486455Z","level":"error","event":"25/07/11 18:02:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:57.824920Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:57.836554Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:57.837062Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:02:59.610187Z","level":"error","event":"25/07/11 18:02:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:03:19.762494","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:03:20.431626Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:03:22.368431Z","level":"error","event":"25/07/11 18:03:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:03:22.633934Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:03:22.643764Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:03:22.644177Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:00.166370","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:04:00.593669Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:02.004492Z","level":"error","event":"25/07/11 18:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:02.207951Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:02.216501Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:02.216906Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:37.120144","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:04:37.503682Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:39.157850Z","level":"error","event":"25/07/11 18:04:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:39.439573Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:39.458312Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:04:39.458953Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:17.121730","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:05:17.616060Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:19.473678Z","level":"error","event":"25/07/11 18:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:19.812154Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:19.812603Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:19.812938Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:55.976210","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:05:56.337773Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:57.544120Z","level":"error","event":"25/07/11 18:05:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:57.711121Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:57.722787Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:05:57.723169Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:06:32.783626","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:06:33.128884Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:06:34.657522Z","level":"error","event":"25/07/11 18:06:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:06:34.840342Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:06:34.861529Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:06:34.861860Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:09.363376","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:07:09.878980Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:11.529542Z","level":"error","event":"25/07/11 18:07:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:11.758432Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:11.758912Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:11.759259Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:48.101171","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:07:48.535787Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:50.272411Z","level":"error","event":"25/07/11 18:07:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:50.463952Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:50.490513Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:07:50.491307Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:08:26.416962","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:08:26.847161Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:08:28.246267Z","level":"error","event":"25/07/11 18:08:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:08:28.461532Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:08:28.471703Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:08:28.472119Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:05.379607","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:09:05.740923Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:07.536791Z","level":"error","event":"25/07/11 18:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:07.776090Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:07.790014Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:07.790525Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:43.971167","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:09:44.504887Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:45.933646Z","level":"error","event":"25/07/11 18:09:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:46.171063Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:46.172229Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:09:46.172736Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:21.526017","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:10:21.901660Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:23.423140Z","level":"error","event":"25/07/11 18:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:23.657809Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:23.669671Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:23.670070Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:10:59.323188","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:10:59.877921Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:01.524268Z","level":"error","event":"25/07/11 18:11:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:01.774080Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:01.784210Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:01.784577Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:37.474898","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:11:37.945990Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:39.591349Z","level":"error","event":"25/07/11 18:11:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:39.771935Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:39.780957Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:11:39.781323Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:18.106001","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:12:18.791650Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:20.376197Z","level":"error","event":"25/07/11 18:12:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:20.624284Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:20.630705Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:20.631205Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:32.403330","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:12:33.157054Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:35.115869Z","level":"error","event":"25/07/11 18:12:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:35.375585Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:35.387019Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:35.387486Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:12:36.459519Z","level":"error","event":"25/07/11 18:12:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:14.641783","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:13:15.339897Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:17.300132Z","level":"error","event":"25/07/11 18:13:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:17.543349Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:17.550951Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:17.551565Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:13:18.829845Z","level":"error","event":"25/07/11 18:13:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:13.706088","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:15:16.216713Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:18.373209Z","level":"error","event":"25/07/11 18:15:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:18.807996Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:18.815945Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:18.816863Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:20.804056Z","level":"error","event":"25/07/11 18:15:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:15:57.221160","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:15:58.179747Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:02.313575Z","level":"error","event":"25/07/11 18:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:02.697624Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:02.708854Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:02.709230Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:43.499343","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:16:44.221841Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:46.204426Z","level":"error","event":"25/07/11 18:16:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:46.490438Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:46.507207Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:16:46.507771Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:17:25.819891","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:17:26.449633Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:17:29.126883Z","level":"error","event":"25/07/11 18:17:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:17:29.394141Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:17:29.401285Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:17:29.401781Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:07.657896","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:18:08.760190Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:10.642513Z","level":"error","event":"25/07/11 18:18:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:10.932174Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:10.932746Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:10.933128Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:50.150450","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:18:51.370410Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:53.522457Z","level":"error","event":"25/07/11 18:18:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:53.961151Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:53.961494Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:18:53.961815Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:33.559714","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:19:34.907207Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:37.227209Z","level":"error","event":"25/07/11 18:19:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:37.542357Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:37.542636Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:37.542889Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:19:38.671968Z","level":"error","event":"25/07/11 18:19:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:00.459195","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:20:01.556620Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:03.512561Z","level":"error","event":"25/07/11 18:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:03.714920Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:03.724756Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:03.725127Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:39.703363","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:20:40.913883Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:43.004861Z","level":"error","event":"25/07/11 18:20:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:43.371660Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:43.387096Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:20:43.387598Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:21:23.841141","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:21:24.637431Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:21:27.261244Z","level":"error","event":"25/07/11 18:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:21:27.571110Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:21:27.571549Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:21:27.571861Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:04.273933","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:22:05.001343Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:06.648646Z","level":"error","event":"25/07/11 18:22:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:06.846745Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:06.860623Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:06.861158Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:33.237026","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:22:33.646916Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:35.042101Z","level":"error","event":"25/07/11 18:22:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:35.219300Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:35.227573Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:35.227980Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:41.229301","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:22:41.683904Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:43.225448Z","level":"error","event":"25/07/11 18:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:43.460491Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:43.470340Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:22:43.470768Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:18.962572","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:23:19.390248Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:20.889188Z","level":"error","event":"25/07/11 18:23:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:21.343677Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:21.350784Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:21.351257Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:23:59.242843","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:23:59.812647Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:01.648534Z","level":"error","event":"25/07/11 18:24:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:01.872924Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:01.899212Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:01.899957Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:38.104598","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:24:38.557007Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:40.071092Z","level":"error","event":"25/07/11 18:24:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:40.263102Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:40.272770Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:24:40.273154Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:16.506318","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:25:16.955841Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:18.584884Z","level":"error","event":"25/07/11 18:25:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:18.793288Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:18.804993Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:18.805483Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:53.846218","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:25:54.268993Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:55.708250Z","level":"error","event":"25/07/11 18:25:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:55.894129Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:55.901909Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:25:55.902214Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:26:31.674317","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:26:32.120714Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:26:33.649764Z","level":"error","event":"25/07/11 18:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:26:33.920198Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:26:33.920970Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:26:33.921337Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:09.075281","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:27:09.471895Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:10.757235Z","level":"error","event":"25/07/11 18:27:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:11.002233Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:11.003056Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:11.003442Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:47.004986","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:27:47.416571Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:48.968131Z","level":"error","event":"25/07/11 18:27:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:49.207592Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:49.218895Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:27:49.219328Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:24.865299","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:28:25.597989Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:27.368975Z","level":"error","event":"25/07/11 18:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:27.594954Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:27.606444Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:27.606863Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:41.289923","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:28:41.735806Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:43.436404Z","level":"error","event":"25/07/11 18:28:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:43.651891Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:43.663064Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:28:43.663450Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:29:21.315873","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:29:21.915519Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:29:24.479803Z","level":"error","event":"25/07/11 18:29:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:29:24.894165Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:29:24.906104Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:29:24.906921Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:04.403227","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:30:05.083072Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:08.270689Z","level":"error","event":"25/07/11 18:30:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:08.807242Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:08.809289Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:08.811485Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:10.917216Z","level":"error","event":"25/07/11 18:30:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:30:34.412865","level":"error","event":"Process timed out, PID: 9103","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T18:30:34.413394","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 9103","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T18:30:34.455303","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T18:31:05.361938","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:31:06.060529Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:08.540516Z","level":"error","event":"25/07/11 18:31:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:08.878507Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:08.888406Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:08.889239Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:10.159313Z","level":"error","event":"25/07/11 18:31:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:47.501765","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:31:48.160788Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:50.432701Z","level":"error","event":"25/07/11 18:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:50.775622Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:50.776134Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:50.776473Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:31:52.659667Z","level":"error","event":"25/07/11 18:31:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:32.297836","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:32:32.979131Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:35.339529Z","level":"error","event":"25/07/11 18:32:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:35.901082Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:35.901739Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:35.902331Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:32:37.495083Z","level":"error","event":"25/07/11 18:32:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:14.549427","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:33:15.221275Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:17.676393Z","level":"error","event":"25/07/11 18:33:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:17.931530Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:17.939917Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:17.940416Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:54.541037","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:33:55.302466Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:57.565180Z","level":"error","event":"25/07/11 18:33:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:57.832720Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:57.833077Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:33:57.833409Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:34:33.868815","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:34:34.402315Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:34:36.827816Z","level":"error","event":"25/07/11 18:34:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:34:37.100698Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:34:37.109240Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:34:37.109750Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:14.310366","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:35:15.047428Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:17.537511Z","level":"error","event":"25/07/11 18:35:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:17.890585Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:17.899620Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:17.900222Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:54.906629","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:35:55.681737Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:57.787936Z","level":"error","event":"25/07/11 18:35:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:58.141875Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:58.149464Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:58.150088Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:35:59.500769Z","level":"error","event":"25/07/11 18:35:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:36:36.079136","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:36:36.980927Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:36:39.110102Z","level":"error","event":"25/07/11 18:36:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:36:39.449160Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:36:39.449450Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:36:39.449785Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:37:19.874325","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:37:20.772909Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:37:23.187686Z","level":"error","event":"25/07/11 18:37:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:37:23.494290Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:37:23.505834Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:37:23.506423Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:02.358775","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:38:03.327522Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:06.136153Z","level":"error","event":"25/07/11 18:38:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:06.428427Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:06.428776Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:06.429222Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:43.474401","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:38:44.068583Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:46.510752Z","level":"error","event":"25/07/11 18:38:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:46.912497Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:46.921510Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:38:46.942658Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:39:23.725894","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:39:24.345250Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:39:26.381213Z","level":"error","event":"25/07/11 18:39:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:39:26.660607Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:39:26.660911Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:39:26.661181Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:02.882937","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:40:03.499536Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:05.588021Z","level":"error","event":"25/07/11 18:40:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:06.049342Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:06.058584Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:06.059242Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:07.214525Z","level":"error","event":"25/07/11 18:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:42.204805","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:40:42.784111Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:44.858901Z","level":"error","event":"25/07/11 18:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:45.117041Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:45.127367Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:45.127898Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:40:46.453865Z","level":"error","event":"25/07/11 18:40:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:22.278330","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:41:22.838130Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:24.921860Z","level":"error","event":"25/07/11 18:41:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:25.270305Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:25.284824Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:25.285378Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:41:26.705040Z","level":"error","event":"25/07/11 18:41:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:03.390191","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:42:03.968148Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:06.706956Z","level":"error","event":"25/07/11 18:42:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:07.094311Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:07.094768Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:07.095119Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:46.083511","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:42:46.805944Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:49.501954Z","level":"error","event":"25/07/11 18:42:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:49.869340Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:49.869741Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:49.870128Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:42:51.226009Z","level":"error","event":"25/07/11 18:42:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:28.500325","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:43:29.044127Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:31.892009Z","level":"error","event":"25/07/11 18:43:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:32.400984Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:32.409892Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:32.410400Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:43:33.710891Z","level":"error","event":"25/07/11 18:43:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:44:15.126073","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:44:15.549229Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:44:17.925988Z","level":"error","event":"25/07/11 18:44:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:44:18.355476Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:44:18.355856Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:44:18.356606Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:01.569028","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:45:02.014123Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:04.403494Z","level":"error","event":"25/07/11 18:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:04.699725Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:04.700018Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:04.700289Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:06.198337Z","level":"error","event":"25/07/11 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:42.599922","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:45:43.093559Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:45.625197Z","level":"error","event":"25/07/11 18:45:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:45.963484Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:45.963861Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:45:45.964335Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:46:23.697872","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:46:24.177703Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:46:26.373903Z","level":"error","event":"25/07/11 18:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:46:26.703557Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:46:26.703804Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:46:26.704106Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:04.467628","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:47:04.949316Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:07.603321Z","level":"error","event":"25/07/11 18:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:07.909735Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:07.917133Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:07.917571Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:44.850660","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:47:45.267634Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:47.928174Z","level":"error","event":"25/07/11 18:47:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:48.182529Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:48.191390Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:48.191951Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:47:49.450273Z","level":"error","event":"25/07/11 18:47:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:48:25.567807","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:48:26.049206Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:48:28.331563Z","level":"error","event":"25/07/11 18:48:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:48:28.667181Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:48:28.675522Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:48:28.676168Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:05.971388","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:49:06.461995Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:08.488686Z","level":"error","event":"25/07/11 18:49:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:08.762719Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:08.770435Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:08.771086Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:09.971177Z","level":"error","event":"25/07/11 18:49:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:44.993900","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:49:45.410047Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:47.755035Z","level":"error","event":"25/07/11 18:49:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:48.034883Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:48.035347Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:48.035662Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:49:49.079380Z","level":"error","event":"25/07/11 18:49:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:24.915714","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:50:25.348768Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:27.507177Z","level":"error","event":"25/07/11 18:50:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:27.787016Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:27.787377Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:27.787701Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:50:28.808081Z","level":"error","event":"25/07/11 18:50:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:04.696583","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:51:05.376593Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:07.888565Z","level":"error","event":"25/07/11 18:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:08.174779Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:08.175288Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:08.175664Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:48.007319","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:51:48.459628Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:50.670602Z","level":"error","event":"25/07/11 18:51:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:51.006437Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:51.007025Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:51:51.007441Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:29.736789","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:52:30.235270Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:32.570725Z","level":"error","event":"25/07/11 18:52:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:32.882673Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:32.883013Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:32.883456Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:52:34.030304Z","level":"error","event":"25/07/11 18:52:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:10.156342","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:53:10.590779Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:12.529013Z","level":"error","event":"25/07/11 18:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:12.829809Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:12.842997Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:12.843680Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:14.020792Z","level":"error","event":"25/07/11 18:53:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:49.568012","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:53:50.087911Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:52.155868Z","level":"error","event":"25/07/11 18:53:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:52.434960Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:52.435516Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:53:52.435896Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:29.491762","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:54:29.979657Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:32.168220Z","level":"error","event":"25/07/11 18:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:32.451527Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:32.460644Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:32.461305Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:54:33.587258Z","level":"error","event":"25/07/11 18:54:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:10.182040","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:55:10.634542Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:13.302910Z","level":"error","event":"25/07/11 18:55:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:13.589460Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:13.590117Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:13.590499Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:15.965471Z","level":"error","event":"25/07/11 18:55:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:51.628589","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:55:52.099753Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:54.191416Z","level":"error","event":"25/07/11 18:55:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:54.488549Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:54.489048Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:54.489444Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:55:55.773544Z","level":"error","event":"25/07/11 18:55:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:31.746674","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:56:32.238218Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:34.210700Z","level":"error","event":"25/07/11 18:56:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:34.486492Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:34.496749Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:34.497324Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:56:35.863827Z","level":"error","event":"25/07/11 18:56:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:12.014222","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:57:12.741724Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:14.606784Z","level":"error","event":"25/07/11 18:57:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:14.867331Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:14.875650Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:14.876217Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:16.131710Z","level":"error","event":"25/07/11 18:57:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:52.652143","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:57:53.555448Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:55.848929Z","level":"error","event":"25/07/11 18:57:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:56.183136Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:56.199085Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:57:56.199776Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:58:34.636028","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:58:35.096448Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:58:37.059259Z","level":"error","event":"25/07/11 18:58:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:58:37.363240Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:58:37.371641Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:58:37.372277Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:14.283637","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:59:14.716081Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:16.433046Z","level":"error","event":"25/07/11 18:59:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:16.656360Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:16.664021Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:16.664483Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:52.906107","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T18:59:53.364926Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:55.182342Z","level":"error","event":"25/07/11 18:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:55.401853Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:55.409427Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:55.409973Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T18:59:56.301519Z","level":"error","event":"25/07/11 18:59:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:31.590393","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:00:31.981127Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:33.892522Z","level":"error","event":"25/07/11 19:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:34.165920Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:34.176143Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:34.176757Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:00:35.496685Z","level":"error","event":"25/07/11 19:00:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:01:01.883693","level":"error","event":"Process timed out, PID: 25995","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:01:02.357338","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 25995","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:01:03.235566","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:01:36.948742","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:01:37.534805Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:01:39.374544Z","level":"error","event":"25/07/11 19:01:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:01:39.628756Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:01:39.642610Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:01:39.643110Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:16.049336","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:02:16.492388Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:18.619587Z","level":"error","event":"25/07/11 19:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:18.886839Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:18.887124Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:18.887365Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:19.973567Z","level":"error","event":"25/07/11 19:02:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:55.364187","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:02:55.790297Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:57.949603Z","level":"error","event":"25/07/11 19:02:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:58.283366Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:58.284054Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:02:58.284671Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:34.674315","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:03:35.118680Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:36.890475Z","level":"error","event":"25/07/11 19:03:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:37.140035Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:37.140339Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:37.140670Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:03:38.199075Z","level":"error","event":"25/07/11 19:03:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:13.824568","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:04:14.251020Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:16.061672Z","level":"error","event":"25/07/11 19:04:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:16.273345Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:16.280287Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:16.280766Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:17.378785Z","level":"error","event":"25/07/11 19:04:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:52.792231","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:04:53.301420Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:55.766007Z","level":"error","event":"25/07/11 19:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:56.372318Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:56.384138Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:56.384721Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:04:59.119026Z","level":"error","event":"25/07/11 19:04:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:22.921344","level":"error","event":"Process timed out, PID: 28303","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:05:23.027498","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 28303","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:05:23.206445","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:05:23.300069Z","level":"error","event":"25/07/11 19:05:23 ERROR Inbox: Ignoring error","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.300799Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.301447Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.302088Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.302876Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.304237Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.311169Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.312945Z","level":"error","event":"\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.313502Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.314394Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.315490Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.316131Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.318201Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.318810Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.319223Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.319705Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.320071Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.320517Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.320848Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.321204Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.321713Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.322123Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.322505Z","level":"error","event":"Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e7d188092ac:40849","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.322856Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.323261Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.323668Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.324088Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.325148Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.325645Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.326034Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.326709Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.327128Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.327625Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.327972Z","level":"error","event":"\t... 17 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.328344Z","level":"error","event":"25/07/11 19:05:23 WARN Executor: Issue communicating with driver in heartbeater","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.328609Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.329020Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.329404Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.329698Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.330486Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.330990Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.333719Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.336449Z","level":"error","event":"\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.337670Z","level":"error","event":"\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.338288Z","level":"error","event":"\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.339032Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.339511Z","level":"error","event":"\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.340215Z","level":"error","event":"\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.352770Z","level":"error","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.354056Z","level":"error","event":"\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.354508Z","level":"error","event":"\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.355107Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.355641Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.356066Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.356721Z","level":"error","event":"Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.357145Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.358833Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.360183Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.360859Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.361225Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.361591Z","level":"error","event":"\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.361912Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.362261Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.362642Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.362995Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.363311Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.363697Z","level":"error","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.364020Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.364323Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.364625Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.365087Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.365344Z","level":"error","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.365844Z","level":"error","event":"\t... 3 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.366427Z","level":"error","event":"Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1e7d188092ac:40849","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.366774Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.367144Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.367472Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.367718Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.368123Z","level":"error","event":"\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.368393Z","level":"error","event":"\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.368792Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.369083Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.369376Z","level":"error","event":"\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.369738Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:23.370053Z","level":"error","event":"\t... 17 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:54.537902","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:05:55.043932Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:57.345482Z","level":"error","event":"25/07/11 19:05:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:57.798153Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:57.810412Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:05:57.810942Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:06:38.075319","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:06:38.552595Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:06:40.840661Z","level":"error","event":"25/07/11 19:06:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:06:41.110395Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:06:41.123788Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:06:41.124231Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:19.456645","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:07:20.076043Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:22.276054Z","level":"error","event":"25/07/11 19:07:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:22.571044Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:22.591137Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:22.591622Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:07:59.820185","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:08:00.298531Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:02.216733Z","level":"error","event":"25/07/11 19:08:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:02.455814Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:02.464091Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:02.464601Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:40.091278","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:08:40.535947Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:42.601340Z","level":"error","event":"25/07/11 19:08:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:42.898690Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:42.899114Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:08:42.899451Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:22.115737","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:09:22.643250Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:25.146744Z","level":"error","event":"25/07/11 19:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:25.511363Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:25.511718Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:25.512036Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:09:52.194643","level":"error","event":"Process timed out, PID: 30542","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:09:52.482765","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 30542","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:09:52.947565","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:10:25.207989","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:10:25.744439Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:10:28.032712Z","level":"error","event":"25/07/11 19:10:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:10:28.387023Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:10:28.396797Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:10:28.397410Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:10:29.472792Z","level":"error","event":"25/07/11 19:10:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:05.081806","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:11:05.526899Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:07.412699Z","level":"error","event":"25/07/11 19:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:07.697215Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:07.697778Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:07.698162Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:45.394194","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:11:45.851020Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:47.963063Z","level":"error","event":"25/07/11 19:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:48.449518Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:48.458310Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:11:48.459160Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:25.541396","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:12:26.098600Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:28.842244Z","level":"error","event":"25/07/11 19:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:29.251192Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:29.266252Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:29.266867Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:12:30.536351Z","level":"error","event":"25/07/11 19:12:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:06.887058","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:13:07.348256Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:10.049442Z","level":"error","event":"25/07/11 19:13:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:10.367437Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:10.367851Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:10.368203Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:48.445810","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:13:48.916357Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:51.117878Z","level":"error","event":"25/07/11 19:13:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:51.399288Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:51.399572Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:51.399863Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:13:52.507965Z","level":"error","event":"25/07/11 19:13:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:27.975072","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:14:28.540310Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:30.506869Z","level":"error","event":"25/07/11 19:14:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:30.787232Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:30.787559Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:30.787875Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:14:31.815993Z","level":"error","event":"25/07/11 19:14:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:07.103343","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:15:07.562432Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:09.815822Z","level":"error","event":"25/07/11 19:15:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:10.075140Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:10.089983Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:10.090502Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:47.690915","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:15:48.276537Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:50.515045Z","level":"error","event":"25/07/11 19:15:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:51.120281Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:51.120563Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:15:51.120813Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:16:29.815878","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:16:30.265820Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:16:32.310044Z","level":"error","event":"25/07/11 19:16:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:16:32.603023Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:16:32.603688Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:16:32.604090Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:11.844665","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:17:12.694877Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:14.662783Z","level":"error","event":"25/07/11 19:17:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:14.915864Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:14.923175Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:14.923623Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:53.362695","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:17:54.317141Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:56.833275Z","level":"error","event":"25/07/11 19:17:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:57.191691Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:57.200801Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:17:57.201359Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:36.330127","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:18:36.977239Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:40.175370Z","level":"error","event":"25/07/11 19:18:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:40.540589Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:40.541305Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:40.542082Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:18:42.706875Z","level":"error","event":"25/07/11 19:18:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:06.476350","level":"error","event":"Process timed out, PID: 35520","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:19:06.687424","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 35520","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:19:07.915858","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:19:40.819997","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:19:41.308986Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:43.366647Z","level":"error","event":"25/07/11 19:19:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:43.711057Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:43.720487Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:43.725053Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:19:45.029023Z","level":"error","event":"25/07/11 19:19:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:22.047067","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:20:22.549920Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:24.987601Z","level":"error","event":"25/07/11 19:20:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:25.297217Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:25.308213Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:25.308798Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:27.561894Z","level":"error","event":"25/07/11 19:20:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:20:52.393069","level":"error","event":"Process timed out, PID: 36290","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:20:52.863841","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 36290","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:20:54.707495","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:21:26.208775","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:21:26.679796Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:21:29.811221Z","level":"error","event":"25/07/11 19:21:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:21:30.338497Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:21:30.342226Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:21:30.342965Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:21:56.376787","level":"error","event":"Process timed out, PID: 36684","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:21:56.742716","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 36684","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":51,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:21:57.310472","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:22:29.096014","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:22:29.676883Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:22:32.420565Z","level":"error","event":"25/07/11 19:22:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:22:32.745341Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:22:32.754801Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:22:32.755371Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:22:34.577034Z","level":"error","event":"25/07/11 19:22:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:11.911228","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:23:12.404216Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:14.637558Z","level":"error","event":"25/07/11 19:23:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:14.892786Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:14.893283Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:14.893667Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:15.869600Z","level":"error","event":"25/07/11 19:23:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:52.271358","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:23:52.735689Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:55.006371Z","level":"error","event":"25/07/11 19:23:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:55.327265Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:55.327615Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:23:55.327949Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:33.172716","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:24:33.698528Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:35.963007Z","level":"error","event":"25/07/11 19:24:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:36.248719Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:36.256934Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:36.257464Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:24:37.328949Z","level":"error","event":"25/07/11 19:24:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:12.984019","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:25:13.416445Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:15.466685Z","level":"error","event":"25/07/11 19:25:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:15.751333Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:15.751655Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:15.752052Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:52.374664","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:25:52.810095Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:54.925649Z","level":"error","event":"25/07/11 19:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:55.175708Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:55.185078Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:55.185718Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:25:56.241521Z","level":"error","event":"25/07/11 19:25:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:31.519030","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:26:32.130726Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:34.582484Z","level":"error","event":"25/07/11 19:26:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:34.933452Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:34.941221Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:34.941701Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:26:36.128249Z","level":"error","event":"25/07/11 19:26:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:12.210220","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:27:12.659522Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:14.864564Z","level":"error","event":"25/07/11 19:27:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:15.211628Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:15.220680Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:15.221174Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:16.579994Z","level":"error","event":"25/07/11 19:27:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:53.825610","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:27:54.422894Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:57.529730Z","level":"error","event":"25/07/11 19:27:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:57.872318Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:57.872901Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:27:57.874590Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:28:23.977146","level":"error","event":"Process timed out, PID: 39959","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:28:25.127183","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 39959","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:28:31.105655","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:29:02.738467","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:29:03.277957Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:06.478480Z","level":"error","event":"25/07/11 19:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:06.911913Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:06.945491Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:06.946054Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:08.970329Z","level":"error","event":"25/07/11 19:29:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:29:36.893375","level":"error","event":"Process timed out, PID: 40261","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:29:39.175318","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 40261","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:29:39.682007","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:30:11.296989","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:32:22.424196","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:32:25.335153Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:32:27.275931Z","level":"error","event":"25/07/11 19:32:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:32:27.742985Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:32:27.753522Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:32:27.754072Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:32:29.619762Z","level":"error","event":"25/07/11 19:32:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:06.086201","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:33:06.567071Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:08.972661Z","level":"error","event":"25/07/11 19:33:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:09.364612Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:09.371839Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:09.372294Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:47.486682","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:33:47.988886Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:50.042755Z","level":"error","event":"25/07/11 19:33:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:50.287807Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:50.296669Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:33:50.297379Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:34:27.575284","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:34:28.047599Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:34:29.670666Z","level":"error","event":"25/07/11 19:34:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:34:29.888358Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:34:29.896065Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:34:29.896504Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:06.272961","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:35:07.591779Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:09.261998Z","level":"error","event":"25/07/11 19:35:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:09.492626Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:09.493211Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:09.493593Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:46.882214","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:35:48.110161Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:35:51.846312Z","level":"error","event":"25/07/11 19:35:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:18.617672","level":"error","event":"Process timed out, PID: 1961","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:36:21.366430","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 1961","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:36:23.541864Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.542449Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.543454Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.769722Z","level":"error","event":"Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmpry0k9tu0/connection7229308170806236780.info","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.771037Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.779795Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.788649Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.790722Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.791383Z","level":"error","event":"\tat java.base/java.nio.file.Files.newByteChannel(Files.java:380)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.791895Z","level":"error","event":"\tat java.base/java.nio.file.Files.createFile(Files.java:658)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.793133Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.793692Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.794178Z","level":"error","event":"\tat java.base/java.nio.file.Files.createTempFile(Files.java:878)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.794751Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.795745Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.796708Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.804563Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.808673Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.809358Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.810775Z","level":"error","event":"\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.812180Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.819884Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.821634Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.824279Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.825593Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.826457Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:36:23.827099Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:07.175324","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:37:08.594115Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:13.510329Z","level":"error","event":"25/07/11 19:37:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:15.392807Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:15.423831Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:15.427669Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:37:37.672467","level":"error","event":"Process timed out, PID: 2160","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:37:39.052071","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 2160","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:37:42.251182","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:38:15.609496","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:38:16.367817Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:38:18.952527Z","level":"error","event":"25/07/11 19:38:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:38:19.297574Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:38:19.307414Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:38:19.308026Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:38:58.213397","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:38:58.661185Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:00.825288Z","level":"error","event":"25/07/11 19:39:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:01.157639Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:01.158002Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:01.158287Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:38.500030","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:39:39.099262Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:41.461690Z","level":"error","event":"25/07/11 19:39:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:41.840980Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:41.856131Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:39:41.856603Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:21.086778","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:40:21.575140Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:23.694327Z","level":"error","event":"25/07/11 19:40:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:24.058146Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:24.097098Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:24.098224Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:40:25.348354Z","level":"error","event":"25/07/11 19:40:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:02.304659","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:41:02.829078Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:05.322989Z","level":"error","event":"25/07/11 19:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:05.613967Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:05.625085Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:05.625608Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:07.117801Z","level":"error","event":"25/07/11 19:41:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:41:32.332219","level":"error","event":"Process timed out, PID: 3903","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:41:32.343634","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 3903","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:41:32.420639","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:42:04.024896","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:42:04.585720Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:06.760985Z","level":"error","event":"25/07/11 19:42:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:07.057530Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:07.066917Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:07.067440Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:08.730293Z","level":"error","event":"25/07/11 19:42:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:46.262931","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:42:46.732366Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:49.003643Z","level":"error","event":"25/07/11 19:42:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:49.323064Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:49.323506Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:42:49.323975Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:43:16.540225","level":"error","event":"Process timed out, PID: 4673","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T19:43:16.996927","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 4673","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T19:43:21.394951","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T19:43:58.157913","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:43:58.833276Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:01.455343Z","level":"error","event":"25/07/11 19:44:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:01.823323Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:01.823814Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:01.824162Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:40.506578","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:44:41.084524Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:43.477310Z","level":"error","event":"25/07/11 19:44:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:43.777260Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:43.785525Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:44:43.786093Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:45:22.005385","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:45:22.509202Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:45:24.451025Z","level":"error","event":"25/07/11 19:45:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:45:24.715996Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:45:24.723762Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:45:24.724249Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:01.672173","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:46:02.176605Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:04.092744Z","level":"error","event":"25/07/11 19:46:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:04.398778Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:04.399045Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:04.399375Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:41.891099","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:46:42.399512Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:44.396969Z","level":"error","event":"25/07/11 19:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:44.715259Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:44.725934Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:44.726555Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:46:46.157588Z","level":"error","event":"25/07/11 19:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:47:21.624567","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:47:22.106487Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:47:24.041105Z","level":"error","event":"25/07/11 19:47:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:47:24.331673Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:47:24.332214Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:47:24.332877Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:01.790278","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:48:02.312381Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:04.532056Z","level":"error","event":"25/07/11 19:48:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:04.864105Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:04.864453Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:04.864767Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:42.364784","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:48:42.812547Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:45.146808Z","level":"error","event":"25/07/11 19:48:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:45.514876Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:45.515380Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:45.515870Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:48:46.802332Z","level":"error","event":"25/07/11 19:48:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:23.825743","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:49:24.325684Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:26.986810Z","level":"error","event":"25/07/11 19:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:27.294508Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:27.306382Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:27.307087Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:49:29.050441Z","level":"error","event":"25/07/11 19:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:06.973543","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:50:07.529586Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:10.345000Z","level":"error","event":"25/07/11 19:50:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:10.666026Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:10.666495Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:10.666799Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:48.663896","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:50:49.221756Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:52.294224Z","level":"error","event":"25/07/11 19:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:52.721294Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:52.771361Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:50:52.771884Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:51:31.926187","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:51:32.459907Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:51:34.962380Z","level":"error","event":"25/07/11 19:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:51:35.251019Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:51:35.251419Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:51:35.251776Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:12.391424","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:52:12.878370Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:14.725755Z","level":"error","event":"25/07/11 19:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:14.972360Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:14.979170Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:14.979659Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:51.428077","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:52:51.851514Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:53.612125Z","level":"error","event":"25/07/11 19:52:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:53.904852Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:53.905213Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:53.905508Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:52:54.883276Z","level":"error","event":"25/07/11 19:52:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:53:30.950809","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:53:31.383376Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:53:33.123304Z","level":"error","event":"25/07/11 19:53:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:53:33.367586Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:53:33.367932Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:53:33.368182Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:09.994899","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:54:10.501839Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:12.262956Z","level":"error","event":"25/07/11 19:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:12.531250Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:12.531676Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:12.532012Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:50.061047","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:54:50.529773Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:52.469740Z","level":"error","event":"25/07/11 19:54:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:52.755607Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:52.756074Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:52.756443Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:54:53.888537Z","level":"error","event":"25/07/11 19:54:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:29.519622","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:55:31.294874Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:33.691612Z","level":"error","event":"25/07/11 19:55:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:33.951986Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:33.965862Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:33.966504Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:55:35.322304Z","level":"error","event":"25/07/11 19:55:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:11.473101","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:56:12.368866Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:14.298758Z","level":"error","event":"25/07/11 19:56:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:14.541430Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:14.541757Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:14.542024Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:52.247571","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:56:52.972975Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:55.183318Z","level":"error","event":"25/07/11 19:56:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:55.473609Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:55.474157Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:56:55.474585Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:57:34.297118","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:57:34.767549Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:57:36.647284Z","level":"error","event":"25/07/11 19:57:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:57:36.933756Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:57:36.941252Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:57:36.941745Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:13.442121","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:58:14.094270Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:16.805661Z","level":"error","event":"25/07/11 19:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:17.126545Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:17.138077Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:17.138630Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:19.433734Z","level":"error","event":"25/07/11 19:58:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:58:57.974416","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:58:58.505755Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:00.778364Z","level":"error","event":"25/07/11 19:59:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:01.075706Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:01.075986Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:01.076270Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:02.188196Z","level":"error","event":"25/07/11 19:59:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:37.894946","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T19:59:38.398985Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:40.525966Z","level":"error","event":"25/07/11 19:59:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:40.825536Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:40.825860Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:40.826116Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T19:59:42.081775Z","level":"error","event":"25/07/11 19:59:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:00:39.115425","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:00:40.523458Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:09.782224","level":"error","event":"Process timed out, PID: 14239","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:01:12.811756","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 14239","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":1236,"name":"poll"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":1983,"name":"_internal_poll"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:01:38.981690","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:01:39.538091Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:41.767003Z","level":"error","event":"25/07/11 20:01:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:41.998549Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:42.005118Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:42.005804Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:01:43.845010Z","level":"error","event":"25/07/11 20:01:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:21.774233","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:02:22.225156Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:24.656865Z","level":"error","event":"25/07/11 20:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:24.984069Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:24.998548Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:24.999042Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:02:26.328189Z","level":"error","event":"25/07/11 20:02:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:01.780808","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:03:02.444845Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:05.847467Z","level":"error","event":"25/07/11 20:03:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:06.315995Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:06.325306Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:06.327212Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:08.351392Z","level":"error","event":"25/07/11 20:03:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:48.864881","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:03:49.597965Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:52.378799Z","level":"error","event":"25/07/11 20:03:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:52.778056Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:52.778599Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:52.779003Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:03:54.393986Z","level":"error","event":"25/07/11 20:03:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:04:32.868925","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:04:33.402795Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:04:36.053425Z","level":"error","event":"25/07/11 20:04:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:04:36.502292Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:04:36.514232Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:04:36.514865Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:14.842766","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:05:15.304007Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:17.316459Z","level":"error","event":"25/07/11 20:05:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:17.639333Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:17.639612Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:17.639903Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:18.872301Z","level":"error","event":"25/07/11 20:05:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:05:54.739861","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:05:57.111729Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:22.257707Z","level":"error","event":"25/07/11 20:06:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:22.765723Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:22.766157Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:22.766637Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:24.416805Z","level":"error","event":"25/07/11 20:06:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:24.757160","level":"error","event":"Process timed out, PID: 2339","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:06:24.759051","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 2339","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:06:24.768833","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:06:55.878132","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:06:56.356340Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:58.538566Z","level":"error","event":"25/07/11 20:06:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:58.834252Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:58.834627Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:06:58.834903Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:07:38.470780","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:07:39.018207Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:07:42.525714Z","level":"error","event":"25/07/11 20:07:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:07:56.172276Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:07:56.183286Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:07:56.183866Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:08.222617Z","level":"error","event":"25/07/11 20:08:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:08.461177","level":"error","event":"Process timed out, PID: 3010","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:08:08.461737","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 3010","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":300,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":429,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:08:08.465045","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:08:39.477793","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:08:40.541516Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:43.294963Z","level":"error","event":"25/07/11 20:08:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:43.660669Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:43.669137Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:08:43.670462Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:21.846025","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:09:22.577969Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:24.530993Z","level":"error","event":"25/07/11 20:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:24.900879Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:24.901259Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:24.901632Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:09:26.256748Z","level":"error","event":"25/07/11 20:09:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:01.783080","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:10:02.252757Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:04.217232Z","level":"error","event":"25/07/11 20:10:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:04.554944Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:04.555407Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:04.555865Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:05.911362Z","level":"error","event":"25/07/11 20:10:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:42.499481","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:10:43.007986Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:45.110300Z","level":"error","event":"25/07/11 20:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:45.570928Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:45.585643Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:45.586232Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:10:46.799570Z","level":"error","event":"25/07/11 20:10:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:22.427103","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:11:22.870500Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:24.956836Z","level":"error","event":"25/07/11 20:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:25.328740Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:25.338579Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:25.339118Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:11:26.615756Z","level":"error","event":"25/07/11 20:11:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:02.383265","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:12:02.839530Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:05.073648Z","level":"error","event":"25/07/11 20:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:05.512610Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:05.513459Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:05.514110Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:06.785334Z","level":"error","event":"25/07/11 20:12:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:43.603146","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:12:44.049718Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:46.362686Z","level":"error","event":"25/07/11 20:12:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:46.665102Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:46.673523Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:12:46.674074Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:13:25.269899","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:13:25.945190Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:13:29.369059Z","level":"error","event":"25/07/11 20:13:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:13:56.988283","level":"error","event":"Process timed out, PID: 5972","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:13:59.350314","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 5972","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:14:07.340334Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.340898Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.341454Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.373949Z","level":"error","event":"Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmp0nspwn1w/connection7160802885329809803.info","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.374235Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.374567Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.374894Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.375187Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.375512Z","level":"error","event":"\tat java.base/java.nio.file.Files.newByteChannel(Files.java:380)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.375794Z","level":"error","event":"\tat java.base/java.nio.file.Files.createFile(Files.java:658)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.376085Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.376442Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.376735Z","level":"error","event":"\tat java.base/java.nio.file.Files.createTempFile(Files.java:878)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.377133Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.377476Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.377741Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.378009Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.378331Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.378915Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.379232Z","level":"error","event":"\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.379603Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.379965Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.380262Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.380549Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.380848Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.381156Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:07.381447Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:44.528033","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:14:45.034279Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:47.403331Z","level":"error","event":"25/07/11 20:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:47.711700Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:47.720879Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:14:47.721388Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:25.569719","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:15:26.096676Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:28.394721Z","level":"error","event":"25/07/11 20:15:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:28.684480Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:28.693581Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:28.694100Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:15:29.850601Z","level":"error","event":"25/07/11 20:15:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:07.313741","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:16:07.793620Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:09.980016Z","level":"error","event":"25/07/11 20:16:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:10.391482Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:10.392447Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:10.392847Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:11.659040Z","level":"error","event":"25/07/11 20:16:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:46.945577","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:16:47.439417Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:49.641249Z","level":"error","event":"25/07/11 20:16:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:50.013737Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:50.021532Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:16:50.022051Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:27.640669","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:17:28.150910Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:30.517760Z","level":"error","event":"25/07/11 20:17:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:30.914257Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:30.926280Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:30.926801Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:17:32.389002Z","level":"error","event":"25/07/11 20:17:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:04.941424","level":"error","event":"Process timed out, PID: 7690","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:18:05.637712","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 7690","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:18:05.773734","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:18:37.592359","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:18:38.171951Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:41.239824Z","level":"error","event":"25/07/11 20:18:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:41.875623Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:41.926936Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:41.927887Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:18:43.457358Z","level":"error","event":"25/07/11 20:18:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:21.536741","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:19:22.098635Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:24.132823Z","level":"error","event":"25/07/11 20:19:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:24.398801Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:24.407273Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:24.407806Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:19:25.656330Z","level":"error","event":"25/07/11 20:19:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:02.605834","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:20:03.120317Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:05.243449Z","level":"error","event":"25/07/11 20:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:05.592149Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:05.592804Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:05.593213Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:43.190517","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:20:43.601541Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:46.160214Z","level":"error","event":"25/07/11 20:20:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:46.582987Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:46.594253Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:46.594853Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:20:47.981523Z","level":"error","event":"25/07/11 20:20:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:21:24.354124","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:21:24.806397Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:21:27.079736Z","level":"error","event":"25/07/11 20:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:21:27.397899Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:21:27.398256Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:21:27.398571Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:06.002842","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:22:33.001568Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:36.025326","level":"error","event":"Process timed out, PID: 9989","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:22:36.028339","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 9989","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":205,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":444,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":108,"name":"launch_gateway"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:22:37.774280Z","level":"error","event":"25/07/11 20:22:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.199720Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.229469Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.230504Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.255624Z","level":"error","event":"Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmp7wa405dt/connection1878042063488895626.info","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.256190Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.256567Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.256928Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.257233Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.257537Z","level":"error","event":"\tat java.base/java.nio.file.Files.newByteChannel(Files.java:380)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.257819Z","level":"error","event":"\tat java.base/java.nio.file.Files.createFile(Files.java:658)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.258109Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.258430Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.258703Z","level":"error","event":"\tat java.base/java.nio.file.Files.createTempFile(Files.java:878)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.259002Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.259344Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.259628Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.259900Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.260190Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.260463Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.260753Z","level":"error","event":"\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.261052Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.261314Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.261677Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.261936Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.262281Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.262603Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:22:38.263014Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:23:35.905766","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:23:36.627755Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:23:38.837209Z","level":"error","event":"25/07/11 20:23:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:23:39.191551Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:23:39.195483Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:23:39.196208Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:06.220534","level":"error","event":"Process timed out, PID: 10193","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:24:07.147632","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 10193","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:24:08.153406","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:24:41.345782","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:24:42.009728Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:44.400484Z","level":"error","event":"25/07/11 20:24:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:44.745039Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:44.753303Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:44.753861Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:24:46.878887Z","level":"error","event":"25/07/11 20:24:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:25:11.612439","level":"error","event":"Process timed out, PID: 10579","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:25:12.479621","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 10579","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:25:14.153516","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:25:46.329991","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:25:47.077838Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:25:49.445340Z","level":"error","event":"25/07/11 20:25:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:25:49.790804Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:25:49.791087Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:25:49.791449Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:20.169926","level":"error","event":"Process timed out, PID: 10965","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:26:29.146375","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 10965","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":556,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":523,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":207,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/core/context.py","lineno":312,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:26:35.381068Z","level":"error","event":"25/07/11 20:26:35 WARN Executor: Issue communicating with driver in heartbeater","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.382000Z","level":"error","event":"org.apache.spark.SparkException: Exception thrown in awaitResult:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.382735Z","level":"error","event":"\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.383737Z","level":"error","event":"\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.384223Z","level":"error","event":"\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.384892Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.385409Z","level":"error","event":"\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1292)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.386299Z","level":"error","event":"\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.387063Z","level":"error","event":"\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.387697Z","level":"error","event":"\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.387995Z","level":"error","event":"\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.388334Z","level":"error","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.389034Z","level":"error","event":"\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.389428Z","level":"error","event":"\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.389795Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.390152Z","level":"error","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.390536Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.391112Z","level":"error","event":"Caused by: org.apache.spark.SparkException: Could not find HeartbeatReceiver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.391480Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:181)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.391825Z","level":"error","event":"\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:146)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.392232Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:245)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.392555Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:561)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.392926Z","level":"error","event":"\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:565)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.393236Z","level":"error","event":"\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:100)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:26:35.393542Z","level":"error","event":"\t... 11 more","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:47.154349","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:28:50.639480Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:52.622925Z","level":"error","event":"25/07/11 20:28:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:53.084786Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:53.108928Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:53.111753Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:28:54.729946Z","level":"error","event":"25/07/11 20:28:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:29:31.169790","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:29:31.629682Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:29:33.582095Z","level":"error","event":"25/07/11 20:29:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:29:33.932075Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:29:33.932395Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:29:33.932724Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:12.552610","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:30:13.131155Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:16.990246Z","level":"error","event":"25/07/11 20:30:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:18.275641Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:18.276004Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:18.276389Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:20.437939Z","level":"error","event":"25/07/11 20:30:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:30:57.064083","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:30:57.776253Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:00.159622Z","level":"error","event":"25/07/11 20:31:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:00.445859Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:00.454172Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:00.454852Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:02.084784Z","level":"error","event":"25/07/11 20:31:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:39.522235","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:31:40.039099Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:42.179784Z","level":"error","event":"25/07/11 20:31:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:42.463508Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:42.471804Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:31:42.472379Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:21.078517","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:32:21.913443Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:24.296643Z","level":"error","event":"25/07/11 20:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:24.711902Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:24.727263Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:24.727824Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:32:26.255064Z","level":"error","event":"25/07/11 20:32:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:04.130033","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:33:05.569598Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:08.486147Z","level":"error","event":"25/07/11 20:33:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:08.786337Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:08.795090Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:08.795763Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:46.550639","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:33:47.013396Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:49.250236Z","level":"error","event":"25/07/11 20:33:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:49.512275Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:49.512610Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:49.512918Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:33:50.535133Z","level":"error","event":"25/07/11 20:33:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:34:27.232870","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:34:27.767698Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:34:29.636309Z","level":"error","event":"25/07/11 20:34:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:34:29.889064Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:34:29.897542Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:34:29.898090Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:06.684999","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:35:07.124327Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:09.114398Z","level":"error","event":"25/07/11 20:35:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:09.386069Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:09.386395Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:09.386647Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:10.458021Z","level":"error","event":"25/07/11 20:35:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:46.473830","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:35:46.912257Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:48.994015Z","level":"error","event":"25/07/11 20:35:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:49.243480Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:49.243844Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:49.244476Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:50.302515Z","level":"error","event":"25/07/11 20:35:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:35:50.303110Z","level":"error","event":"25/07/11 20:35:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:25.741278","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:36:26.191325Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:28.307988Z","level":"error","event":"25/07/11 20:36:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:28.550890Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:28.560091Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:28.560533Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:36:29.602324Z","level":"error","event":"25/07/11 20:36:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:05.546308","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:37:05.986203Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:08.374097Z","level":"error","event":"25/07/11 20:37:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:08.667664Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:08.668072Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:08.668433Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:09.851904Z","level":"error","event":"25/07/11 20:37:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:47.169719","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:37:48.175016Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:51.443731Z","level":"error","event":"25/07/11 20:37:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:51.842979Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:51.851428Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:51.851975Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:37:53.385838Z","level":"error","event":"25/07/11 20:37:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:31.644028","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:38:32.237484Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:34.722309Z","level":"error","event":"25/07/11 20:38:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:35.066011Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:35.066370Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:35.066755Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:38:36.550781Z","level":"error","event":"25/07/11 20:38:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:39:04.149934","level":"error","event":"Process timed out, PID: 5423","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-07-11T20:39:06.521743","level":"error","event":"Failed to import: /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/etl_data_weather.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 5423","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":15,"name":"<module>"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":55,"name":"get_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":559,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":641,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1626,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":535,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-07-11T20:39:09.439031","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-07-11T20:39:42.844142","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:39:43.446365Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:39:45.932213Z","level":"error","event":"25/07/11 20:39:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:39:46.277287Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:39:46.285885Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:39:46.286447Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:23.510170","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:40:23.971604Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:26.124022Z","level":"error","event":"25/07/11 20:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:26.421167Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:26.421678Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:26.422092Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:40:27.656926Z","level":"error","event":"25/07/11 20:40:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:05.064051","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:41:05.553881Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:07.789188Z","level":"error","event":"25/07/11 20:41:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:08.122313Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:08.137346Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:08.137824Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:44.585148","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:41:45.074929Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:47.480883Z","level":"error","event":"25/07/11 20:41:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:47.783834Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:47.784180Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:47.784518Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:41:49.027619Z","level":"error","event":"25/07/11 20:41:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:42:25.108721","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:42:25.581111Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:42:27.847048Z","level":"error","event":"25/07/11 20:42:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:42:28.118767Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:42:28.119326Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:42:28.119732Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:06.236233","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:43:06.719784Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:08.991624Z","level":"error","event":"25/07/11 20:43:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:09.290349Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:09.307747Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:09.308317Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:10.885830Z","level":"error","event":"25/07/11 20:43:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:47.836682","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:43:48.281529Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:50.526434Z","level":"error","event":"25/07/11 20:43:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:50.802204Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:50.810745Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:43:50.811422Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:44:27.968178","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/etl_data_weather.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-11T20:44:28.465245Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:44:30.722122Z","level":"error","event":"25/07/11 20:44:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:44:31.027800Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:44:31.035669Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-07-11T20:44:31.036155Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
